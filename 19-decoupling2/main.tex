
\documentclass{article}
\usepackage{nips_2018,graphicx}

 \usepackage[english]{babel} 
  \usepackage{times}		
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} 
\DeclareUnicodeCharacter{00A0}{~}
\usepackage{amssymb,amsmath,amscd,amsfonts,amsthm,bbm,mathrsfs,yhmath}
\usepackage[shortlabels]{enumitem}
\usepackage{hyperref}

\usepackage[textwidth=2cm, textsize=footnotesize]{todonotes}  
\setlength{\marginparwidth}{1.5cm}               %  this goes with todonotes
\newcommand{\aknote}[1]{\todo[color=cyan!20]{#1}}
\newcommand{\asnote}[1]{\todo[color=green!]{#1}}
\newcommand{\adil}[1]{\todo[inline]{\textbf{Adil: }#1}}
\newcommand{\peter}[1]{\todo[inline]{\textbf{Peter: }#1}}

\newcommand{\cF}{{\mathcal F}} 
\newcommand{\cS}{{\mathcal S}} 
\newcommand{\cM}{{\mathcal M}} 
\newcommand{\cP}{{\mathcal P}} 
\newcommand{\cO}{{\mathcal O}} 

\DeclareMathOperator{\prox}{prox}


\newcommand{\cC}{{\mathcal C}} 
\newcommand{\E}{{{\mathbb E}}} 
\newcommand{\bR}{{\mathbb R}} 
\newcommand{\bP}{{\mathbb P}} 
\newcommand{\bE}{{\mathbb E}} 
\newcommand{\sX}{{\mathsf X}} 
\newcommand{\sY}{{\mathsf Y}} 

\newcommand{\cL}{{{\mathcal L}}}



\newcommand{\KL}{\mathop{\mathrm{KL}}\nolimits}
\newcommand{\tr}{\mathop{\mathrm{tr}}\nolimits}
\newcommand{\ps}[1]{\langle #1 \rangle}
\newcommand{\Supp}{\mathop{\mathrm{Supp}}\nolimits}


\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{condition}{Condition}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}

\title{From $\sigma_k$ to Decoupling}


\begin{document}

\maketitle

\begin{abstract} 

\end{abstract}

Generally, the goal of this work is to go from the analysis of SGD proposed in the $\sigma_k$ paper~\cite{gorbunov2019unified} to the decoupling paper~\cite{mishchenko2019stochastic}.

In~\cite{gorbunov2019unified}, SGD is analyzed under the following assumptions 
\begin{assumption}
Unbiasedness
\begin{equation}
    \bE[g^k|x^k] = \nabla f(x^k)
\end{equation}
\end{assumption}

We are especially trying to understand the following points.
\begin{itemize}
    \item What is the fundamental reasons behind the linear rates obtained in~\cite{mishchenko2019stochastic} (Start with the case where $m=1$)
    \item \cite{gorbunov2019unified} provide a tight analysis of SGD, can we generalize it to the setting of~\cite{mishchenko2019stochastic}
    \item If we particularize~\cite{mishchenko2019stochastic} to the setting of SGD, what kind of analysis do we get? Especially, it allows biased estimate of the gradient.
    \item In~\cite{mishchenko2019stochastic}, what is the assumption made on $v_t$ ? what is $Y_t$?
%    \item Minibatch analysis of~\cite{mishchenko2019stochastic}
\end{itemize}

\section{Extension of~\cite{gorbunov2019unified} to the weakly convex setting and $R$ strongly convex}
Let $F$ (resp. $R$) $\mu$ strongly convex (resp $\lambda$ strongly convex). We allow $\mu=0$ (resp $\lambda = 0$)  
The prox-SGD algorithm can be written
\begin{equation}
    x^{k+1} = x^k - \gamma g^k - \gamma \partial R(x^{k+1})
\end{equation}
The symbol $=$ should be an inclusion $\in$ since $\partial R$ is set valued, but we keep $=$ to simplify.

The iterates $x^k$ satisfy

\begin{align}
    \label{eq:PG-final}
    &\|x^{k+1} - x^\star\|^2 \\
    =& \|x^k - x^\star\|^2 -\gamma^2\|\partial R(x^{k+1}) - \partial R(x^\star)\|^2 + \gamma^2 \|g^k - \nabla F(x^\star) \|^2\\
    &- 2\gamma\ps{\partial R(x^{k+1})-\partial R(x^\star),x^{k+1}-x^\star} - 2\gamma\ps{g^k-\nabla F(x^\star),x^k-x^\star}.
\end{align}
This equality will be proven in the sequel in a more general case (see Eq.~\eqref{eq:FB-final}).

We assume that $g^k$ satisfy (\cite{gorbunov2019unified}): there exist an adapted stochastic process $(\sigma_k)_k$ and constants $A,B,C \geq 0$ such that
\begin{align}
    \bE_k(\|g^k - \nabla f(x^\star)\|^2) &\leq 2AD_F(x^k,x^\star) + B\sigma_k + D_1\\
    \bE_k(\sigma_{k+1}^2) &\leq (1-\rho)\sigma_k^2 + 2CD_F(x^k,x^\star) + D_2,
\end{align}
where $\bE_k$ denotes the conditional expectation.

From Eq.~\eqref{PG-final},
\begin{align}
    &\bE_k\|x^{k+1} - x^\star\|^2 \\
    \leq& \|x^k - x^\star\|^2 + \gamma^2 \bE_k\|g^k - \nabla F(x^\star) \|^2\\
    &- 2\gamma \bE_k D_R(x^{k+1},x^\star) - 2\gamma D_F(x^{k},x^\star)\\
    \leq& (1-\gamma \mu)\|x^k - x^\star\|^2 - \gamma \lambda \|x^{k+1} - x^\star\|^2\\
    &- 2\gamma \bE_k D_R(x^{k+1},x^\star) - 2\gamma(1 - \gamma A) D_F(x^{k},x^\star)\\
    &+ \gamma^2 B \sigma_k + \gamma^2 D_1.
\end{align}
Therefore, noting that $\frac{1-\gamma\mu}{1+\gamma\lambda} = 1-\gamma\frac{\mu+\lambda}{1+\gamma\lambda}$,
\begin{align}
    &\bE_k (1+\gamma \lambda)\|x^{k+1} - x^\star\|^2 + M(1+\gamma\lambda)\gamma^2 \bE_k \sigma_{k+1} + \bE_k 2\gamma D_R(x^{k+1},x^\star)\\
    \leq& (1-\gamma\mu)\|x^k - x^\star\|^2 + 2\gamma \frac{1-\gamma\mu}{1+\gamma\lambda} D_R(x^{k},x^\star)\\
    &- 2\gamma \left(1-\gamma\frac{\mu+\lambda}{1+\gamma\lambda}\right) D_R(x^{k},x^\star) - 2\gamma(1 - \gamma (A + M(1+\gamma\lambda)C)) D_F(x^{k},x^\star)\\
    &+ \gamma^2 M \left((1-\rho)(1+\gamma\lambda) + \frac{B}{M}\right)\sigma_k + \gamma^2 (D_1 + M(1+\gamma\lambda)D_2).
\end{align}

Let $$V^k = \|x^{k} - x^\star\|^2 + M\gamma^2 \sigma_{k} +  \frac{2\gamma}{1+\gamma\lambda} D_R(x^{k},x^\star),$$ $$\alpha = \max\{1-\gamma\mu, (1-\rho)(1+\gamma\lambda) + \frac{B}{M}\},$$
and 
$$
\beta = \max\{\frac{\mu+\lambda}{1+\gamma\lambda},A + M(1+\gamma\lambda)C\}
$$

\begin{align}
    (1+\gamma\lambda)\bE_k V^{k+1}
    \leq& \alpha V^k - 2\gamma (1-\gamma\beta) (D_R(x^{k},x^\star) + D_F(x^{k},x^\star))\\
    &+\gamma^2 (D_1 + M(1+\gamma\lambda)D_2)\\
    =& \alpha V^k - 2\gamma (1-\gamma\beta) ((F+R)(x^k) - (F+R)(x^\star))\\
    &+\gamma^2 (D_1 + M(1+\gamma\lambda)D_2),
\end{align}
where the last equality comes from $\nabla F(x^\star) + \partial R(x^\star) = 0$.

One can take $\gamma$ small enough in order to have $1-\gamma\beta >0$ (note that $\gamma \beta \to 0$ as $\gamma \to 0$). Once such $\gamma$ is chosen, one can take $M$ large enough in order to have $1-\rho + \frac{B}{M(1+\gamma\lambda)} < 1$ (for example $M > B/\rho$).


If $\mu >0$ or $\lambda >0$, $V^k$ converges linearly with the rate $\frac{\alpha}{1+\gamma\lambda}$, up to a neighborhood.

In the weakly convex setup ($\mu = 0 =\lambda$) we have 

\begin{equation}
    \label{eq:weakly-cvx-sigmak}
    \bE_k V^{k+1} \leq V^k - 2\gamma(1-\gamma\beta)((F+R)(x^k) - (F+R)(x^\star)) +\gamma^2 (D_1 + M(1+\gamma\lambda)D_2).
\end{equation}
Therefore we have $((F+R)(\overline{x^k}) - (F+R)(x^\star)) = \cO(1/k)$ if $D_1 = 0 = D_2$ (\textit{i.e} if the algorithm is variance reduced)\asnote{And even without averaging if we know that $F+R(x^k)$ is nonincreasing. Show this!} and $((F+R)(\overline{x^k}) - (F+R)(x^\star)) = \cO(1/\sqrt{k})$ else (in the finite horizon setting where the constant step size is chosen as a function of $k$).


\section{About Decoupling}
The Decoupling method~\cite{mishchenko2019stochastic} is a primal dual algorithm, even the Lyapunov function of the Decoupling method without VR is the squared norm of the primal-dual variables ($Y_t$ represents the dual variables)


We shall look at a variant of the Decoupling method of~\cite{mishchenko2019stochastic}.
The Decoupling method~\cite{mishchenko2019stochastic} is written (we use Moreau's identity for the last two lines)

\begin{align}
    x^{k+1}   &= \prox_{\gamma R}(x^k - \gamma v^k - \gamma y^k)\\
    \widetilde{x^{k+1}} &= x^{k+1}\\
    y_j^{k+1} &= \prox_{\frac{1}{\gamma_j} g_j^\star}(y_j^k + \frac{1}{\gamma_j}\widetilde{x^{k+1}} ), \quad j \sim p_j.
\end{align}

We introduce the Decoupling 2 method:
% \adil{Douglas Rachford algorithm is a particular case of Decoupling and Douglas Rachford-normalized form is a particular case of Decoupling2. Since the two form of DR are equivalent, are Decoupling and Decoupling2 equivalent? I don't think so because }

\begin{align}
    x^{k+1}   &= \prox_{\gamma R}(x^k - \gamma v^k - \gamma y^k)\\
    \widetilde{x^{k+1}} &= 2x^{k+1} - x^k\\
    y_j^{k+1} &= \prox_{\frac{1}{\gamma_j} g_j^\star}(y_j^k + \frac{1}{\gamma_j}\widetilde{x^{k+1}} ), \quad j \sim p_j.
\end{align}

If $m=1$ and if $v^k$ is the true gradient, the Decoupling 2 method is Vu Condat algorithm~\cite{con-jota13}:
\begin{align}
    x^{k+1}   &= \prox_{\gamma R}(x^k - \gamma \nabla f(x^k) - \gamma y^k)\\
    y^{k+1} &= \prox_{\frac{1}{\gamma} g^\star}(y^k + \frac{1}{\gamma}(2x^{k+1}-x^k)).
\end{align}



Roadmap.
\begin{itemize}
    \item First, we introduce Vu-Condat algorithm and provide conditions under which this algorithm converges linearly
    \item Then, we replace the gradient appearing in Vu-Condat by a variance reduced unbiaise estimate. The conditions for linear convergence don't change.
    \item Finally, we do coordinate descent for Vu-Condat-VR. This method called, Decoupling 2, is similar to the Decoupling method~\cite{mishchenko2019stochastic}, with similar guarantees. 
\end{itemize}

\section{Vu-Condat}
\subsection{Forward-Backward}
Consider an Euclidean space $\sX$ (it is important to consider an abstract space and not just $\bR^d$ to simplify the presentation), two monotone operators $A,B$ and a positive definite matrix $P$.
The Forward Backward algorithm is written
\begin{equation}
    \label{eq:FB}
    x^{k+1} = x^k - \gamma P^{-1} (B(x^k) + A(x^{k+1}))
\end{equation}
Note that $x^{k+1}$ is implicitly defined. The equality $=$ is actually an inclusion $\in$ because monotone operators can be set-valued, but we assume for simplicity that $A,B$ are single valued (but it is not necessary).

The goal of the FB algorithm is to converge to a zero of $A+B$ i.e a point $x^\star$ such that $0 = A(x^\star) + B(x^\star)$. The most famous application of the FB algorithm is the case where $P=I$, $B = \nabla F$ and $A = \partial G$, where $F$ and $G$ are two convex functions, and $F$ is smooth. In this case the FB algorithm boils down to the prox-grad algorithm and converges to a minimizer of $F+G$.

If $P = I$, the analysis of the FB algorithm is starts follows:
\begin{align*}
    &\|x^{k+1} - x^\star\|^2 \\=& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k),x^k-x^\star} - 2\gamma\ps{A(x^{k+1}),x^k-x^\star} +  \gamma^2\|B(x^k) + A(x^{k+1})\|^2\\ 
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star} - 2\gamma\ps{A(x^{k+1})-A(x^\star),x^k-x^\star} +  \gamma^2\|B(x^k) + A(x^{k+1})\|^2\\
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}  +  \gamma^2\|B(x^k) + A(x^{k+1})\|^2\\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star} -2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k}-x^{k+1}}\\
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}  +  \gamma^2\|B(x^k) + A(x^{k+1})\|^2\\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star} -2\gamma^2\ps{A(x^{k+1})-A(x^\star),B(x^k)+A(x^{k+1})}\\
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}  +  \gamma^2\|B(x^k)\|^2 - \gamma^2\|A(x^{k+1})\|^2 \\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star} +2\gamma^2\ps{A(x^\star),B(x^k)+A(x^{k+1})}.
\end{align*}
The simplifications is the last line comes from expanding the square. Now we use 
$$
\gamma^2\|B(x^k)\|^2 - \gamma^2\|A(x^{k+1})\|^2 +2\gamma^2\ps{A(x^\star),B(x^k)+A(x^{k+1})} = -\gamma^2\|A(x^{k+1}) - A(x^\star)\|^2 + \gamma^2 \|B(x^k) + A(x^\star) \|^2,
$$
To get 
\begin{align*}
    &\|x^{k+1} - x^\star\|^2 \\
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}  -\gamma^2\|A(x^{k+1}) - A(x^\star)\|^2 + \gamma^2 \|B(x^k) + A(x^\star) \|^2\\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star}.
\end{align*}
Finally, we use
$$B(x^k) + A(x^\star) = B(x^k) - B(x^\star) + B(x^\star) + A(x^\star) = B(x^k) - B(x^\star)$$
and we have 
\begin{align}
    \label{eq:FB-final}
    &\|x^{k+1} - x^\star\|^2 \\
    =& \|x^k - x^\star\|^2 -\gamma^2\|A(x^{k+1}) - A(x^\star)\|^2 + \gamma^2 \|B(x^k) - B(x^\star) \|^2\\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star} - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}.
\end{align}
The two inner product are negative because of monotonicity. The only nonnegative term is $\|B(x^k) - B(x^\star) \|^2$. It is managed using some smoothness condition. To get a linear rate, we need a contraction. This contraction can come from the three negative terms. For example, if $A$ or $B$ is strongly monotone we get a contraction. But this is not the only case where we have a contraction, we can use some partial strong monotonicity (we will see an example later).

The analysis in the case $P \neq I$ follows the same lines, by replacing $\|\cdot\|$ (resp. $\ps{\cdot,\cdot}$) by the norm (resp. the inner product) induced by $P$. 




\subsection{Vu-Condat as instance of Forward Backward}

Vu-Condat algorithm aims at minimizing $\min F(x) + G(x) + H(Mx)$ where $F,G,H$ are convex functions, $F$ is smooth and $M : \sX \to \sY$ is a matrix. It is a splitting algorithm: the user is only required to compute $\nabla F$, $\prox_G$ and $\prox_H$ separately. It is a primal dual algorithm that operates in the primal dual space $\sX \times \sY$. In other words, the iterates $(x^k,y^k)$ are elements of $\sX \times \sY$. If $F=0$, Vu-Condat boils down to Chambolle-Pock algorithm.

The iterations of Vu Condat are written
\begin{align}
    x^{k+1}   &= \prox_{\gamma G}(x^k - \gamma \nabla F(x^k) - \gamma M^T y^k)\\
    y^{k+1} &= \prox_{\frac{1}{\gamma} H^\star}(y^k + \frac{1}{\gamma}M(2x^{k+1}-x^k)).
\end{align}

Interestingly, Vu-Condat can be obtained as an instance of the FB algorithm by carefully instanciating $A,B,P$. In the case of Vu-Condat, the optimization space is $\sX \times \sY$ i.e the iterates are elements $(x,y) \in \sX \times \sY$. The operators are defined on $\sX \times \sY$ by
\[
P_\gamma = \begin{bmatrix} I &  -\gamma M^T \\ -\gamma M & I \end{bmatrix},
\]
\[
A(x,y) = \begin{bmatrix} \partial G(x)&  + M^T y \\ -M x& +\partial H^\star(y)\end{bmatrix} .
\]
\[
B(x,y) = \begin{bmatrix} \nabla F(x) \\ 0 \end{bmatrix} .
\]
To see that $A,B$ are monotone, note that $B$ is actually the gradient of the convex function $(x,y) \mapsto F(x)$ and that $A$ is the sum of a skew symmetrix matrix $L$ and the subdifferential of $(x,y) \mapsto G(x) + H^\star(y)$. Recall that a skew symmetric matrix is a monotone operator (since $\ps{Lz,z} = -\ps{Lz,z}$ we have $\ps{Lz,z} = 0 \geq 0$). As a sum, $A$ is monotone.

Note also that a zero $(x^\star,y^\star)$ of $A+B$ is a primal-dual optimal point, i.e a saddle point of the Lagragien. Therefore $x^\star$ is a solution of our problem: if $0 = A(x^\star) + B(x^\star)$ then $0 = -Mx^\star +\partial H^\star(y^\star)$ and therefore $y^\star = \partial H(Mx^\star)$. Moreover $0 = \nabla F(x^\star) + \partial G(x^\star) + M^Ty^\star = \nabla F(x^\star) + \partial G(x^\star) + M^T \partial H(Mx^\star)$, therefore $x^\star$ is a minimizer of $F+G+H \circ M$.

\subsection{Conditions under whcich Vu Condat converges linearly}
The sum of the negative term in the analysis of Forward Backward is (see Eq.~\eqref{eq:FB-final})
$$
\chi^{k+1} = -\gamma^2\|A(z^{k+1}) - A(z^\star)\|^2 - 2\gamma\ps{A(z^{k+1})-A(z^\star),z^{k+1}-z^\star} - 2\gamma\ps{B(z^k)-B(z^\star),z^k-z^\star},
$$
In the case of the monotone operators of Vu Condat, we have
(recall that the norm is the norm on the product space $\sX \times \sY$, and we denote $z^k = (x^k,y^k)$ and $z^\star = (x^\star,y^\star)$)
    \begin{itemize}
        \item $\ps{A(z^{k+1})-A(z^\star),x^{k+1}-x^\star} = \ps{\partial G(x^{k+1})-\partial G(x^\star),x^{k+1}-x^\star}_\sX + \ps{\partial H^\star(y^{k+1})-\partial H^\star(y^\star),y^{k+1}-y^\star}_\sY$
        \item $\ps{B(z^k)-B(z^\star),z^k-z^\star} = \ps{\nabla F(x^k) - \nabla F(x_\star),x-x^\star}_\sX$
        \item $\|A(z^{k+1}) - A(z^\star)\|^2 = \|\partial G(x^{k+1}) + M^T y^{k+1} - (\partial G(x^{\star}) + M^T y^{\star})\|_\sX^2 + \|(-M x^{k+1} + \partial H^\star(y^{k+1})) - (-M x^{\star} + \partial H^\star(y^{\star}))\|_\sY^2$
    \end{itemize}

To have linear rate we need contraction in $x$ and $y$. Each of these term can bring a contraction in $x$ or $y$. 

Here we give some sufficient conditions. We have linear rate if 
    \begin{enumerate}
        \item \label{item:smooth} $F$ strongly convex (or $G$ strongly convex) and $H^\star$ strongly convex (i.e $H$ smooth)
        \item \label{item:lin}$F$ strongly convex (or $G$ strongly convex) and $M^T = I$ (more generally, $M^T \geq \omega I$ \asnote{I mean $\|M^Tx\| \geq \omega\|x\|$ for some $\omega$ which is actually equivalent to $M^T$ injective or $M$ surjective}) and $G = 0$ (more generally, $G$ smooth)
        \item \dots
        \item \dots
        \item \dots
        \item \dots\asnote{Complete}
    \end{enumerate}


    \section{Vu-Condat with VR}
    In Eq.~\eqref{eq:FB-final} we replace the monotone operators $A,B$ by their values in the case of Vu-Condat algorithm.  In this particular case,~\eqref{eq:FB-final} boils down to:
\begin{align*}
    \|x^{k+1} - x^\star\|_\sX^2 + \|y^{k+1} - y^\star\|_\sX^2 
    =& \|x^k - x^\star\|_\sX^2 + \|y^k - y^\star\|_\sY^2 \\
    &-\gamma^2\|\partial G(x^{k+1}) + M^T y^{k+1} - (\partial G(x^{\star}) + M^T y^{\star})\|_\sX^2 \\
    &- \gamma^2\|(-M x^{k+1} + \partial H^\star(y^{k+1})) - (-M x^{\star} + \partial H^\star(y^{\star}))\|_\sY^2 \\
    &+ \gamma^2 \|\nabla F(x^k) - \nabla F(x_\star) \|_\sX^2\\
    &- 2\gamma\ps{\partial G(x^{k+1})-\partial G(x^\star),x^{k+1}-x^\star}_\sX \\
    &- 2\gamma\ps{\partial H^\star(y^{k+1})-\partial H^\star(y^\star),y^{k+1}-y^\star}_\sY \\
    &- 2\gamma\ps{\nabla F(x^k) - \nabla F(x_\star),x-x^\star}_\sX.
\end{align*}
Denote 
\begin{align*}
    \chi^{k+1} 
    =& \gamma\|\partial G(x^{k+1}) + M^T y^{k+1} - (\partial G(x^{\star}) + M^T y^{\star})\|_\sX^2 \\
    & +\gamma\|(-M x^{k+1} + \partial H^\star(y^{k+1})) - (-M x^{\star} + \partial H^\star(y^{\star}))\|_\sY^2 \\
    &+ 2\ps{\partial G(x^{k+1})-\partial G(x^\star),x^{k+1}-x^\star}_\sX \\
    &+ 2\ps{\partial H^\star(y^{k+1})-\partial H^\star(y^\star),y^{k+1}-y^\star}_\sY.
\end{align*}
Assume from now on the every gradient $\nabla F(x^k)$ is replaced by an unbiased estimate $g^k$. We have


\begin{align*}
    \|x^{k+1} - x^\star\|_\sX^2 + \|y^{k+1} - y^\star\|_\sX^2 
    =& \|x^k - x^\star\|_\sX^2 + \|y^k - y^\star\|_\sY^2 \\
    &+ \gamma^2 \|g^k - \nabla F(x_\star) \|_\sX^2\\
    &- 2\gamma\ps{g^k - \nabla F(x_\star),x-x^\star}_\sX\\
    &-\gamma \chi^{k+1}.
\end{align*}

Moreover, we assume that $g^k$ is variance reduced in the sense of~\cite{gorbunov2019unified}: there exist an adapted stochastic process $(\sigma_k)_k$ and constants $A,B,C \geq 0$ such that
\begin{align}
    \bE_k(\|g^k - \nabla f(x^\star)\|^2) &\leq 2AD_F(x^k,x^\star) + B\sigma_k\\
    \bE_k(\sigma_{k+1}^2) &\leq (1-\rho)\sigma_k^2 + 2CD_F(x^k,x^\star),
\end{align}
where $\bE_k$ denotes the conditional expectation.


We can mimic the proof of Theorem 1 in~\cite{gorbunov2019unified} to get a contraction for $\|x^k-x^\star\|^2$.
Taking conditional expectation and using $\mu$-quasi strong convexity we have

\begin{align*}
    \bE_k \|x^{k+1} - x^\star\|_\sX^2 + \|y^{k+1} - y^\star\|_\sX^2 
    \leq& (1-\gamma\mu)\|x^k - x^\star\|_\sX^2 + \|y^k - y^\star\|_\sY^2 \\
    &+ \gamma^2 (2AD_F(x^k,x^\star) + B\sigma_k) - 2\gamma D_F(x^k,x^\star)\\
    &-\gamma\chi^{k+1}.
\end{align*}

Therefore, if $M>0$ is large enough in order to have $1-\rho+B/M < 1$,

\begin{align*}
    &\bE_k \|x^{k+1} - x^\star\|_\sX^2 + \|y^{k+1} - y^\star\|_\sX^2 + M\gamma^2\bE(\sigma_{k+1}^2)\\
    \leq& (1-\gamma\mu)\|x^k - x^\star\|_\sX^2 + \|y^k - y^\star\|_\sY^2 -2\gamma(1-A\gamma)D_F(x^k,x^\star) + B\gamma^2\bE(\sigma_k^2)\\
     &+ M\gamma^2(1-\rho)\bE(\sigma_k^2)+ 2M\gamma^2CD_F(x^k,x^\star)\\
    &-\gamma\chi^{k+1}\\
    =& (1-\gamma\mu)\|x^k - x^\star\|_\sX^2 + \|y^k - y^\star\|_\sY^2 -2\gamma(1-\gamma(A+MC))D_F(x^k,x^\star)\\
     &+ M\gamma^2(1-\rho+\frac{B}{M})\bE(\sigma_k^2)\\
    &-\gamma\chi^{k+1}.
\end{align*}
Finally, denoting $\alpha = \max(1-\rho+B/M,1-\gamma\mu) < 1$ and $W^k = \|x^k-x^\star\|^2 + M\gamma^2\sigma^k$ we have

\asnote{Weakly convex case?}

\begin{align*}
    &\bE W^{k+1} + \bE \|y^{k+1} - y^\star\|_\sX^2\\
    \leq& \alpha W^k + \bE\|y^k - y^\star\|_\sY^2 -2\gamma(1-\gamma(A+MC))D_F(x^k,x^\star)\\
     &-\gamma\chi^{k+1}.
\end{align*}

Now, one can use $\chi^k$ to also have a contraction in $\bE\|y^k - y^\star\|_\sY^2$. \cite{Put expectation everywhere}

\section{An instance of Vu Condat}

Consider the problem 
\begin{equation}
    \label{eq:appli}
\min_{x \in \sX} f(x) + \sum_{j=1}^m g_j(x) + R(x)
\end{equation}
This problem can be tackled by Vu-Condat by setting $F(x) = f(x)$, $\sY = \sX^m$, $H(y_1,\ldots,y_m) = \sum_j g_j(y_j)$ and $M : \sX \to \sY, Mx = 1/m (x,\ldots,x)$ and $G(x) = R(x)$.
The iterates are now elements $(x,(y_1,\ldots,y_m)) \in \sX \times \sX^m$. Note that $G$ is separable and $M^T(y_1,\ldots,y_m) = \frac{1}{m}\sum_{j=1}^m y_j = \overline{y}$. The prox of $G$ can be computed in parallel, therefore Vu-Condat applied to~\ref{eq:appli} is written\asnote{Check the step sizes. Rewrite decoupling as Vu-Condat}
    \begin{align}
        x^{n+1}   &= \prox_{\gamma R}(x^n - \gamma v^n - \gamma \frac{1}{m}\sum_{j=1}^m y_j^n)\\
        \widetilde{x^{n+1}} &= 2x^{n+1} - x^n\\
        y_j^{n+1} &= \prox_{\frac{1}{\gamma_j} g_j^\star}(y_j^n + \frac{1}{\gamma_j}\widetilde{x^{n+1}} ), \forall j.
    \end{align}
    
The value of $\chi^{k+1}$ for this instance of Vu-Condat is 
\begin{align*}
    \chi^{k+1} 
    =& \gamma\|\partial R(x^{k+1}) + \overline{y^{k+1}} - (\partial R(x^{\star}) + \overline{y^{\star}})\|_\sX^2 \\
    & +\sum_{j=1}^m \gamma  \|(-\frac{1}{m}x^{k+1} + \partial g_j^\star(y_j^{k+1})) - (-x^{\star} + \partial g_j^\star(y_j^{\star}))\|_\sX^2 \\
    &+ 2\ps{\partial R(x^{k+1})-\partial R(x^\star),x^{k+1}-x^\star}_\sX \\
    &+ 2 \sum_j \ps{\partial g_j^\star(y_j^{k+1})-\partial g_j^\star(y_j^\star),y_j^{k+1}-y_j^\star}_\sX.
\end{align*}

\begin{itemize}
    \item If $F$ is strongly convex we have a contraction in $x$. If moreover, $g_j$ is smooth for every $j$, we have a contraction in $y_j$ (because in this case $g_j^\star$ is strongly convex). This si the case in Th 6 of\cite{mishchenko2019stochastic}.
    \item If $R = 0$ (or smooth) and if there exists orthogonal\asnote{Supplementaire sufficient? All what we need is linear independence of $(y_j)_j$} vector subspaces $S_1,\ldots,S_m$ of $\sX$ such that for every $k$, $y_j^k \in S_j$, then the first term in $\chi^{k+1}$ is equal to $\frac{1}{m^2}\|y^{k+1} - y^\star\|_{\sX^m}^2$ and we have contraction in $y$. This is the case in Th 4 and 5 of~\cite{mishchenko2019stochastic}\asnote{Not exactly but this is the idea. Precise in order to extend}
\end{itemize}
We now add coordinate descent on top of this method in order to obtain the Decoupling method. The conditions under which linear rate holds don't change.



\section{Coordinate descent}


\section{Possible extensions}

More general linear operator $M$

\subsubsection*{References}
\renewcommand\refname{\vskip -1cm}
\bibliographystyle{apalike}
\bibliography{math}

\end{document}