
\documentclass{article}
\usepackage{nips_2018,graphicx}

 \usepackage[english]{babel} 
  \usepackage{times}		
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} 
\DeclareUnicodeCharacter{00A0}{~}
\usepackage{amssymb,amsmath,amscd,amsfonts,amsthm,bbm,mathrsfs,yhmath}
\usepackage[shortlabels]{enumitem}
\usepackage{hyperref}

\usepackage[textwidth=2cm, textsize=footnotesize]{todonotes}  
\setlength{\marginparwidth}{1.5cm}               %  this goes with todonotes
\newcommand{\aknote}[1]{\todo[color=cyan!20]{#1}}
\newcommand{\asnote}[1]{\todo[color=green!]{#1}}
\newcommand{\adil}[1]{\todo[inline]{\textbf{Adil: }#1}}
\newcommand{\peter}[1]{\todo[inline]{\textbf{Peter: }#1}}

\newcommand{\cF}{{\mathcal F}} 
\newcommand{\cS}{{\mathcal S}} 
\newcommand{\cM}{{\mathcal M}} 
\newcommand{\cP}{{\mathcal P}} 
\newcommand{\cO}{{\mathcal O}} 

\DeclareMathOperator{\prox}{prox}
\newcommand{\sA}{{\mathsf A}}
\newcommand{\sB}{{\mathsf B}}
\newcommand{\maxmon}{{\mathscr M}} 
\DeclareMathOperator{\dom}{dom}
\newcommand{\eqdef}{:=} 


\newcommand{\cC}{{\mathcal C}} 
\newcommand{\E}{{{\mathbb E}}} 
\newcommand{\bR}{{\mathbb R}} 
\newcommand{\bP}{{\mathbb P}} 
\newcommand{\bE}{{\mathbb E}} 
\newcommand{\sX}{{\mathsf X}} 
\newcommand{\sY}{{\mathsf Y}} 

\newcommand{\cL}{{{\mathcal L}}}



\newcommand{\KL}{\mathop{\mathrm{KL}}\nolimits}
\newcommand{\tr}{\mathop{\mathrm{tr}}\nolimits}
\newcommand{\ps}[1]{\langle #1 \rangle}
\newcommand{\Supp}{\mathop{\mathrm{Supp}}\nolimits}


\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{condition}{Condition}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}

\title{Non asymptotic analysis of Stochastic Primal Dual algorithms}


\begin{document}

\maketitle

\begin{abstract} 
\end{abstract}




Generally, the goal of this work is to go from the analysis of SGD proposed in the $\sigma_k$ paper~\cite{gorbunov2019unified} to the decoupling paper~\cite{mishchenko2019stochastic}.

In~\cite{gorbunov2019unified}, SGD is analyzed under the following assumptions 
\begin{assumption}
Unbiasedness
\begin{equation}
    \bE[g^k|x^k] = \nabla f(x^k)
\end{equation}
\end{assumption}

We are especially trying to understand the following points.
\begin{itemize}
    \item What is the fundamental reasons behind the linear rates obtained in~\cite{mishchenko2019stochastic} (Start with the case where $m=1$)
    \item \cite{gorbunov2019unified} provide a tight analysis of SGD, can we generalize it to the setting of~\cite{mishchenko2019stochastic}
    \item If we particularize~\cite{mishchenko2019stochastic} to the setting of SGD, what kind of analysis do we get? Especially, it allows biased estimate of the gradient.
    \item In~\cite{mishchenko2019stochastic}, what is the assumption made on $v_t$ ? what is $Y_t$?
%    \item Minibatch analysis of~\cite{mishchenko2019stochastic}
\end{itemize}

\section{Introduction}

We provide a primal dual interpretation of the Decoupling algorithm. The algorithm is a Variance reduced of the Coodinate Update Vu Condat (CUVC) algorithm. The CUVC is studied in~\cite{Fercoq,Yin}. It uses stochastic coordinate updates~\cite{Iutzeler, Yin, Pesquet Combettes} in top of Vu-Condat algorithm~\cite{Vu, Condat, Chambolle-Pock}.
This interpretation allows us to prove results related to the Decoupling2 algorithm, an algorithm close to the Decoupling algorithm.

The guarantees for the Decoupling2 algorithm are similar to the guarantees of the Decoupling algorithm~\cite{mishchenko2019stochastic}. 

As a special case, we find the results contained in~\cite{gorbunov2019unified} for prox-SGD as well as now ones. 
As a special case, we find known results related to CUVC as well as now ones.

Proofs in the Appendix.

\section{Problem}

Consider the following minimization problem
\begin{equation}
    \label{eq:pb-original}
    \min F(x) + R(x) + \sum_{j=1}^m h_j(A_j x)
\end{equation}
where $F,G,h_j \in \Gamma_0(\bR^d)$ and $F$ is smooth.

This minimization problem can be addressed using Vu-Condat algorithm~\cite{vu-condat}. 

Let $x^0,y_1^0,\ldots,y_m^0$.

\begin{align}
    \label{Vu-Condat}
    x^{k+1}   &= \prox_{\gamma R}(x^k - \gamma \nabla F(x^k) - \gamma \sum_{j=1}^m A_j^{T} y_j^k)\\
    \widetilde{x^{k+1}} &= 2x^{k+1} - x^k\\
    y_j^{k+1} &= \prox_{\frac{1}{\gamma_j} h_j^\star}(y_j^k + \frac{1}{\gamma_j}A_j\widetilde{x^{k+1}} ), \forall j.
\end{align}

We consider the following setting. The use have access to 

\begin{enumerate}
    \item Unbiaised estimate $g^{k}$ of the gradient $\nabla F(x^k)$ :$\bE(g^k|x^k) = \nabla F(x^k)$
    \item Unbiased estimate $r$ of the function $R$: $\bE_\xi(r(x,\xi)) = R(x)$ 
    \item Unbiased estimates $p_j$ of the functions $h_j^\star$: $\bE_\xi(p_j(y,\xi)) = h_j^\star(y)$
    \item For every $j$, a sequence $(a_j^k)_k$ of the matrices such that: \begin{itemize}
                                                                                        \item $(a_j^k)_k$ is a bounded martingale
                                                                                        \item $\bE(a_j^0) = A_j$
    \end{itemize}
\end{enumerate}  

To solve Problem~\eqref{eq:pb-original} in this setting, we introduce the Decoupling2 algorithm. Consider a sequence of iid realizations $\xi^k$ of $\xi$,


\begin{align}
    \label{eq:algo-general}
    x^{k+1}   &= \prox_{\gamma r(\cdot,\xi^{k})}(x^k - \gamma g^{k} - \gamma \sum_{j=1}^m (a_j^{k})^{T} y_j^k)\\
    \widetilde{x^{k+1}} &= 2x^{k+1} - x^k\\
    y_j^{k+1} &= \prox_{\frac{1}{\gamma} p_j(\cdot,\xi^k)}(y_j^k + \frac{1}{\gamma} a_j^k \widetilde{x^{k+1}}), \quad j \sim p_j.
\end{align}

This algorithm is general. Five levels of randomness. It admits relevant (new and old) algorithms as special cases.

\section{Relationship to existing and nonexisting methods}

\subsection{Existing methods}
If all estimate are exact (non random) and if the $m$ dual steps are done in parallel, then the algorithm~\eqref{eq:algo-general} boils down to Vu-Condat in the equivalent form given in~\eqref{eq:Vu-Condat}
If moreover $m=1$, the algorithm~\eqref{eq:algo-general} boils down to Vu-Condat in its classical form.


In the case $m=1$, if $r$ is deterministic and $h_j = 0$ then the algorithm is the prox SGD~\cite{gorbunov2019unified}. If $r$ is also random, it is stochastic prox SGD~\cite{bia-hac,bia-hac-sal}. If moreover $a_1^k$ is also random, then the algorithm is close to~\cite{salim} that allows to solve composite stochastic optimization algorithm under stochastic linear constraints (constraints written as expectations). We will provide more details on it later on.

In the case $m > 1$, if everything else is deterministic, our algorithm is CUVC~\cite{fercoq,yin,pesquet,iutzeler}. If moreover $g^k$ is random, then the algorithm is very close to the Decoupling algorithm~\cite{mishchenko2019stochastic} in the particular case $A_j = I$. The Decoupling algorithm can be written (using Moreau's identity):


\begin{align}
    x^{k+1}   &= \prox_{\gamma R}(x^k - \gamma g^k - \gamma \sum_j y_j^k)\\
    \widetilde{x^{k+1}} &= x^{k+1}\\
    y_j^{k+1} &= \prox_{\frac{1}{\gamma_j} g_j^\star}(y_j^k + \frac{1}{\gamma_j}\widetilde{x^{k+1}} ), \quad j \sim p_j.
\end{align}

The difference with this algorithm is that it sets $\widetilde{x^{k+1}} = x^{k+1}$.


In the sequel we provide four different theorems under various assumptions on the randomness and the function class considered (smooth, convex...).
For each theorem, we provide relevant special cases in which our assumptions hold.
The proof of these results is presented in the appendix. It employs monotone operator theory, more precisely random monotone operators~\cite{bia-hac}. The relationship between Vu-Condat and the monotone operator theory was already noticed in~\cite{Vu-Condat}.


\asnote{General form of Vu-Condat with inf convolution???}

\section{Non VR, weakly cvx}

We consider here our most general assumptions.

We assume that $g^k$ satisfy (\cite{gorbunov2019unified}): there exist an adapted stochastic process $(\sigma_k)_k$ and constants $A,B,C \geq 0$ such that
\begin{align}
    \bE_k(\|g^k - \nabla f(x^\star)\|^2) &\leq 2AD_F(x^k,x^\star) + B\sigma_k + D_1\\
    \bE_k(\sigma_{k+1}^2) &\leq (1-\rho)\sigma_k^2 + 2CD_F(x^k,x^\star) + D_2,
\end{align}
where $\bE_k$ denotes the conditional expectation.

\begin{theorem}
    Consider the following Lagrangian function associated Problem~\eqref{eq:pb-original}: $L(x,y_1,\ldots,y_m) = (F+R)(x) - \sum_j h_j^\star(y_j) + \ps{A_j x, y_j}$. Let $x^\star, y^\star = (y_1^\star,\ldots,y_m^\star)$ a primal dual optimal point.
    Then \begin{equation}
        L(\overline{x^k},y^\star) - L(x^\star,\overline{y^k}) = \ldots = \cO(1/\sqrt{k}).
    \end{equation}
\end{theorem}

\subsection{Relevant special case}

Consider the following problem involving stochastic linear constraints:

\begin{equation}
    \label{eq:sto-lin-cons}
    \min F(x) + R(x) \quad \text{s.t.} \quad Ax = b,
\end{equation}
 in the following setting:
 \begin{enumerate}
    \item $m=1$
    \item Unbiaised estimate $g^{k}$ of the gradient $\nabla F(x^k)$ :$\bE(g^k|x^k) = \nabla F(x^k)$
    \item Unbiased estimate $r$ of the function $R$: $\bE_\xi(r(x,\xi)) = R(x)$ 
    \item Unbiased estimates $b^k$ of the vector $b$: $\bE(b^k|x^k) = b$
    \item A sequence $(a^k)_k$ of the matrices such that: \begin{itemize}
                                                                                        \item $(a^k)_k$ is a bounded martingale
                                                                                        \item $\bE(a^0) = A_j$.
    \end{itemize}
\end{enumerate}  

This setting fits our general framework with $m=1$, $h_1 = \iota_b$ and $A_1 = A$. Indeed, in this case
\begin{equation}
    \iota_b^\star(y) = \ps{y,b} = \bE_k(\ps{y,b^k}) = \bE(p(y,\xi^k))
\end{equation}
where $p(y,\xi^k) = \ps{y,b^k}$ \asnote{not clear}.
Note that in this case, $\prox_{\sigma p(\cdot,\xi^k)}(y) = y - \sigma b^k$.

Therefore the algorithm is in this case:

\begin{align}
    \label{eq:algo-general}
    x^{k+1}   &= \prox_{\gamma r(\cdot,\xi^{k})}(x^k - \gamma g^{k} - \gamma (a^{k})^{T} y^k)\\
    \widetilde{x^{k+1}} &= 2x^{k+1} - x^k\\
    y^{k+1} &= y^k + \frac{1}{\gamma} (a^k \widetilde{x^{k+1}} - b^k).
\end{align}

\subsection{Sanity check: weakly convex theory of~\cite{gorbunov2019unified}}

In the case (Prox-SGD) $m=1$, $A_1 = 0$, $h_1 = 0$ and $r(\cdot,\xi) = R$, the problem is 
$$
\min F+R,
$$
the algorithm
$$
x^{k+1} = \prox{\gamma R}(x^k - \gamma g^k),
$$
and the result
$$
(F+R)(\overline{x^k}) - (F+R)(x^\star) \leq ... = \cO(1/\sqrt{k}).
$$
\asnote{Check that this match what we obtain with the dorect proof of the weakly cvx theory of~\cite{gorbunov}}

\section{Non VR, strongly cvx}

\begin{theorem}
Assume moreover that $A_j$ deterministic, $F$ or $r(\cdot,\xi)$ is strongly convex a.s. and that $p(\cdot,\xi)$ is strongly convex a.s. (\textit{i.e} $p(\cdot,\xi)^\star$) is smooth.
Then 
\begin{equation}
    \bE \|x^k - x^\star\|^2 + \|y^k - y^\star\|^2 \leq \rho^k \bE \|x^0 - x^\star\|^2 + \|y^0 - y^\star\|^2 + C, 
\end{equation} 
where $\rho = $ and $C = $
\end{theorem}
We can reformulate this convergence in terms of Wasserstein distance. Denote $\mu^k$ the distribution of $(x^k,y^k)$, $\mu^\gamma$ an invariant measure of the Markov Chain $(x^k,y^k)$ (which exists because of Hayek) and $\mu^star = \delta_{(x^\star,y^\star)}$ (unique $(x^\star,y^\star)$ because of strong convex-concavity).

\begin{theorem}
    \begin{equation}
        W^2(\mu^k,\mu^\gamma) \leq \rho^k W^2(\mu^0,\mu^\gamma)
    \end{equation}
    and the neighborhood size is
    \begin{equation}
        W^2(\mu^\gamma,\mu^\star) \leq C\gamma
    \end{equation}
\end{theorem}

\subsection{Relevant special case}

$h_j$ smooth and deterministic, $A_j$ deterministic for all $j$

\subsection{Sanity check}

We recover the result of~\cite{gorbunov2019unified} if $r$ is not strongly convex and $m=1$, $h_1=0$, $A_j = 0$.

\section{VR, weakly cvx}

Following~\cite{gorbunov2019unified}, $g^k$ is said variance reduced if $D_1 = 0 = D_2$.
In this section we assume that $g^k$ is VR, and that $r(\cdot,\xi) = R$, and that $p_j$ deterministic and $A_j$ deterministic (or martingale)

\begin{corollary}
If 
\end{corollary}

\subsection{Sanity check}

\section{VR, strongly cvx}



\begin{corollary}
If moreover, $F$ or $R$ is strongly cvx and $A_j$ deterministic and $h_j$ smooth or $h_j = \iota_{b_j}$ or injectivity.
\end{corollary}

\subsection{Special case}



\subsection{Sanity check}

\subsection{Application : linear rate in distributed optimization without strong monotonicity}





\part{Appendix}



\section{Maximal monotone operators}
We review some basic material regarding maximal monotone operators. The proof of these facts can be found in~\cite{bau-com-livre11}.
Let $\sX$ be an Euclidean space and let $I$ be the identity map over $\sX$. An operator $\sA$ over $\sX$ is a set valued mapping over $\sX$, \textit{i.e} a function from $\sX$ to the set of all subsets of $\sX$. An operator can be identified to its graph $G(\sA) = \{(x,y) \in \sX \times \sX, y \in \sA(x)\}$. The domain of $\sA$ is $\dom(\sA) = \{x \in \sX, \sA(x) \neq \emptyset\}$. The inverse operator $\sA^{-1}$ is defined by $G(\sA^{-1}) = \{(y,x) \in \sX \times \sX, y \in \sA(x)\}$, the resolvent operator is defined by $J_{\sA} = (I+\sA)^{-1}$ and the set of zeros of $\sA$ is $Z(\sA) = \sA^{-1}(0)$. Note that $\ell \in Z(\sA)$ if and only if $\ell \in J_\sA(\ell)$. The operator $\sA$ is said monotone if the following condition holds: 
\begin{equation*}
\forall (x,y),(x',y') \in G(\sA), \ps{x-x',y-y'} \geq 0,
\end{equation*}
where $\ps{\cdot,\cdot}$ denotes the inner product of $\sX$. In this case, $J_{\sA}(x)$ is either the empty set or a singleton, \textit{i.e} $J_{\sA}$ can be identify with a classical function $\dom(J_{\sA}) \to \sX$.
Moreover, $\sA$ is a maximal monotone operator, which we denote $\sA \in \maxmon(H)$, if $\sA$ is a monotone operator such that $\dom(J_{\sA}) = H$. In this case, for every $x$, the inclusion $y \in x - \gamma A(y)$ (where $y$ is the unknown) has a unique solution $y = J_{\sA}(x)$. If $y' = J_{\sA}(x')$, then
$$
\|y-y'\|^2 = \|x-x'\|^2 - 2\gamma\ps{A(y)-A(y'),y-y'} - \gamma^2\|A(y)-A(y')\|^2,
$$
with a slight notation abuse.
The maximality of $\sA$ is equivalent to the maximality (for the inclusion ordering) of $G(\sA)$ in the set of all graphs of monotone operators over $H$~\cite{minty1962monotone}. Moreover, if $\sA$ is maximal, then for every $x \in H$, $\sA(x)$ is a (possibly empty) closed convex set. If $x \in \dom(\sA)$, $\sA_0(x)$ is defined as the projection of $0$ onto $\sA(x)$. In other words, $\sA_0(x)$ is the least norm element in $\sA(x)$. Given $\gamma > 0$, the Yosida approximation of $\sA$ is the function $\sA_\gamma(x) = \frac{x - J_{\gamma\sA}(x)}{\gamma}$. The function $\sA_\gamma$ is $1/\gamma$-Lipschitz continuous. Given two maximal monotone operators $\sA$ and $\sB$, the sum $\sA + \sB$ is defined by $(\sA + \sB)(x) \eqdef \sA(x) + \sB(x)$ where $\sA(x) + \sB(x)$ is the classical Minkowski sum of two sets. One can check that $\sA + \sB$ is a monotone operator, however, $\sA + \sB$ is not necessarily maximal~\cite[Page 54]{phelps2009convex}. 

Consider the set $\Gamma_0(H)$ of convex lower semi-continuous and proper functions $F : H \to (-\infty,+\infty]$ (see~\cite{bau-com-livre11}). Then, the subdifferential $\partial F$ of $F$ is a maximal monotone operator. In other words, $\maxmon_s(H) = \{\partial F, F \in \Gamma_0(H)\}$ is a subset of $\maxmon(H)$. Moreover, $J_{\gamma \partial F} = \prox_{\gamma F}$. Let $C$ be a convex set and consider $F = \iota_C$ the convex indicator function of $C$, defined by $F(x) = 0$ if $x \in C$ and $F(x) = +\infty$ else. Then $F \in  \in \Gamma_0(H)$ and $\partial F$ is the normal cone $N_C$ to $C$. 
% However, if $\dom(\sA) = H$, then $\sA + \sB$ is maximal\cite[Th. ]{bre-livre73}. In the case where $\sA = \partial F$ and $\partial G$ where $F,G \in \Gamma_0(H)$, the condition $\dom(\sA) = H$ implies $\sA + \sB = \partial(F+G)$.
\asnote{inverse of subdifferential = subdiff of Legendre transfrom}
\section{Vu-Condat}

Given two maximal monotone operators $\sA,\sB$, the goal of the Forward Backward algorithm is to find a zero of $\sA + \sB$.
The Forward Backward algorithm is written 
\begin{equation}
    \label{eq:FB}
    x^{k+1} = J_{\gamma\sA}(x^k - \gamma \sB(x^k))
\end{equation}
The FB can be equivalently defined by the inclusion
\begin{equation}
    \label{eq:FB2}
    x^{k+1} \in x^k - \gamma\sB(x^k) - \gamma\sA(x^{k+1}).
\end{equation}


Under some qualification conditions, a minimizer $x^\star$ of Problem~\eqref{eq:pb-original} is a solution to the inclusion
$$
0 \in \nabla F(x^\star) + \partial R(x^\star) + \sum_j A_j^T \partial h_j (A_j x^\star).
$$
In other words, for every $j$ there exists $y_j^\star \in \partial h_j (A_j x^\star)$ such that
$$
0 \in \nabla F(x^\star) + \partial R(x^\star) + \sum_j A_j^T y_j^\star.
$$
We can reformulate this property as follows: the tuple $(x^\star,y_1^\star,\ldots,y_m^\star)$ must satisfy
\begin{align}
    \label{eq:pridu-opt}
    0 &\in \nabla F(x^\star) + \partial R(x^\star)+ \sum_j A_j^T y_j^\star\\
    0 &\in -A_j x^\star + \partial h_j^\star(y_j^\star),
\end{align}
for every $j$.
To solve this inclusion, a first idea could be to apply the Forward Backward algorithm to the following monotone operators (note that $\sA$ is a monotone operator under the Euclidean inner product as a sum of a subdifferential and a skew symmetric matrix and that $\sB$ is the gradient of $(x,y) \mapsto F(x)$):
\[
\sA(x,y) = \begin{bmatrix} \partial R(x)&  + \sum_j A_j^T y_j \\ -A_j x& +\partial h_j^\star(y_j)\end{bmatrix} .
\]
\[
\sB(x,y) = \begin{bmatrix} \nabla F(x) \\ 0 \end{bmatrix} .
\]
Indeed, $0 \in \sA+\sB(x^\star,y^\star)$ iff $(x^\star,y^\star)$ satisfy~\eqref{eq:pridu-opt}.
The iterates of the FB algorithm $(x^k,y^k)$ converge  to a solution $(x^\star,y^\star)$ provided $F$ is smooth. However, it can be seen that the update is intractable in many cases.
To overcome this issue, the idea of Vu and Condat is to apply the FB algorithm to the monotone operators $P_\gamma^{-1} \sA$ and $P_\gamma^{-1} \sB$ in the Euclidean space $\bR^d \times \bR^{d_1} \times \ldots \times \bR^{d_m}$ with the inner product induced by $P_{\gamma}$ defined by

\[
P_\gamma = \begin{bmatrix} I &  -\gamma M^T \\ -\gamma M & I \end{bmatrix},
\]
Note that $(1-\gamma\|M\|)\|z\| \leq \ps{P_\gamma z,z} \leq (1+\gamma\|M\|)\|z\|$ and hence $P_\gamma$ is regular as soon as $\gamma < 1/\|M\|$

The reason for doing this is twofold.
First, $P_\gamma^{-1} \sA$ and $P_\gamma^{-1} \sB$ are monotone operators :
$$
\ps{P_\gamma^{-1} \sA(x) - P_\gamma^{-1} \sA(y),x-y}_{P_\gamma} = \ps{\sA(x) - \sA(y),x-y} \geq 0
$$
because $\sA$ is a monotone operator under the Euclidean inner product.
Second, the update of the FB algorithm only require the computation of $\prox_{\gamma R}, \prox_{\gamma h_j}, \nabla F$:
$$z^{k+1} - z^k \in -\gamma P_\gamma^{-1} \sB(z^k) - \gamma P_\gamma^{-1} \sA(z^{k+1})$$
is equivalent to 
$$
P_\gamma (z^{k+1} - z^k) \in -\gamma \sB(z^k) - \gamma \sA(z^{k+1}),
$$
which gives Vu-Condat algorithm~\eqref{eq:vc}.


\section{Proof of the VR theorems}
In Vu-Condat, we replace $\sB$ (the gradient of $(x,y) \mapsto \nabla F(x)$) by some unbiased estimate $G^k = (g^k,0)$. 
Let $F$ (resp. $R$) $\mu$ strongly convex (resp $\lambda$ strongly convex). We allow $\mu=0$ (resp $\lambda = 0$)  
The algorithm is written
\begin{equation}
    z^{k+1} = z^k - \gamma P_\gamma^{-1}G^k - \gamma P_\gamma^{-1} A(z^{k+1})
\end{equation}
The symbol $=$ should be an inclusion $\in$ since $A$ is set valued, but we keep $=$ to simplify.

The iterates $x^k$ satisfy (see Eq.~\eqref{eq:resolvent})

\begin{align}
    \label{eq:PG-final}
    &\|z^{k+1} - z^\star\|_{P_\gamma}^2 \\
    =& \|z^k - z^\star\|_{P_\gamma}^2 -\gamma^2\|P_\gamma^{-1} A(z^{k+1}) - P_\gamma^{-1} A(z^\star)\|_{P_\gamma}^2 + \gamma^2 \|G^k - \nabla F(x^\star) \|_{P_\gamma}^2\\
    &- 2\gamma\ps{A(z^{k+1})-A(z^\star),z^{k+1}-z^\star}_{P_\gamma} - 2\gamma\ps{g^k-\nabla F(x^\star),x^k-x^\star}_{P_\gamma}.
\end{align}
This equality will be proven in the sequel in a more general case (see Eq.~\eqref{eq:FB-final}).
\asnote{Don't forget to use negative suare norm for strongly monotone case/linear case}
We assume that $g^k$ satisfy (\cite{gorbunov2019unified}): there exist an adapted stochastic process $(\sigma_k)_k$ and constants $A,B,C \geq 0$ such that
\begin{align}
    \bE_k(\|g^k - \nabla f(x^\star)\|^2) &\leq 2AD_F(x^k,x^\star) + B\sigma_k + D_1\\
    \bE_k(\sigma_{k+1}^2) &\leq (1-\rho)\sigma_k^2 + 2CD_F(x^k,x^\star) + D_2,
\end{align}
where $\bE_k$ denotes the conditional expectation.

From Eq.~\eqref{PG-final},
\begin{align}
    &\bE_k\|x^{k+1} - x^\star\|^2 \\
    \leq& \|x^k - x^\star\|^2 + \gamma^2 \bE_k\|g^k - \nabla F(x^\star) \|^2\\
    &- 2\gamma \bE_k D_R(x^{k+1},x^\star) - 2\gamma D_F(x^{k},x^\star)\\
    \leq& (1-\gamma \mu)\|x^k - x^\star\|^2 - \gamma \lambda \|x^{k+1} - x^\star\|^2\\
    &- 2\gamma \bE_k D_R(x^{k+1},x^\star) - 2\gamma(1 - \gamma A) D_F(x^{k},x^\star)\\
    &+ \gamma^2 B \sigma_k + \gamma^2 D_1.
\end{align}
Therefore, noting that $\frac{1-\gamma\mu}{1+\gamma\lambda} = 1-\gamma\frac{\mu+\lambda}{1+\gamma\lambda}$,
\begin{align}
    &\bE_k (1+\gamma \lambda)\|x^{k+1} - x^\star\|^2 + M(1+\gamma\lambda)\gamma^2 \bE_k \sigma_{k+1} + \bE_k 2\gamma D_R(x^{k+1},x^\star)\\
    \leq& (1-\gamma\mu)\|x^k - x^\star\|^2 + 2\gamma \frac{1-\gamma\mu}{1+\gamma\lambda} D_R(x^{k},x^\star)\\
    &- 2\gamma \left(1-\gamma\frac{\mu+\lambda}{1+\gamma\lambda}\right) D_R(x^{k},x^\star) - 2\gamma(1 - \gamma (A + M(1+\gamma\lambda)C)) D_F(x^{k},x^\star)\\
    &+ \gamma^2 M \left((1-\rho)(1+\gamma\lambda) + \frac{B}{M}\right)\sigma_k + \gamma^2 (D_1 + M(1+\gamma\lambda)D_2).
\end{align}

Let $$V^k = \|x^{k} - x^\star\|^2 + M\gamma^2 \sigma_{k} +  \frac{2\gamma}{1+\gamma\lambda} D_R(x^{k},x^\star),$$ $$\alpha = \max\{1-\gamma\mu, (1-\rho)(1+\gamma\lambda) + \frac{B}{M}\},$$
and 
$$
\beta = \max\{\frac{\mu+\lambda}{1+\gamma\lambda},A + M(1+\gamma\lambda)C\}
$$

\begin{align}
    (1+\gamma\lambda)\bE_k V^{k+1}
    \leq& \alpha V^k - 2\gamma (1-\gamma\beta) (D_R(x^{k},x^\star) + D_F(x^{k},x^\star))\\
    &+\gamma^2 (D_1 + M(1+\gamma\lambda)D_2)\\
    =& \alpha V^k - 2\gamma (1-\gamma\beta) ((F+R)(x^k) - (F+R)(x^\star))\\
    &+\gamma^2 (D_1 + M(1+\gamma\lambda)D_2),
\end{align}
where the last equality comes from $\nabla F(x^\star) + A(x^\star) = 0$.

One can take $\gamma$ small enough in order to have $1-\gamma\beta >0$ (note that $\gamma \beta \to 0$ as $\gamma \to 0$). Once such $\gamma$ is chosen, one can take $M$ large enough in order to have $1-\rho + \frac{B}{M(1+\gamma\lambda)} < 1$ (for example $M > B/\rho$).


If $\mu >0$ or $\lambda >0$, $V^k$ converges linearly with the rate $\frac{\alpha}{1+\gamma\lambda}$, up to a neighborhood.

In the weakly convex setup ($\mu = 0 =\lambda$) we have 

\begin{equation}
    \label{eq:weakly-cvx-sigmak}
    \bE_k V^{k+1} \leq V^k - 2\gamma(1-\gamma\beta)((F+R)(x^k) - (F+R)(x^\star)) +\gamma^2 (D_1 + M(1+\gamma\lambda)D_2).
\end{equation}
Therefore we have $((F+R)(\overline{x^k}) - (F+R)(x^\star)) = \cO(1/k)$ if $D_1 = 0 = D_2$ (\textit{i.e} if the algorithm is variance reduced)\asnote{And even without averaging if we know that $F+R(x^k)$ is nonincreasing. Show this!} and $((F+R)(\overline{x^k}) - (F+R)(x^\star)) = \cO(1/\sqrt{k})$ else (in the finite horizon setting where the constant step size is chosen as a function of $k$).


\section{About Decoupling}
The Decoupling method~\cite{mishchenko2019stochastic} is a primal dual algorithm, even the Lyapunov function of the Decoupling method without VR is the squared norm of the primal-dual variables ($Y_t$ represents the dual variables)


We shall look at a variant of the Decoupling method of~\cite{mishchenko2019stochastic}.
The Decoupling method~\cite{mishchenko2019stochastic} is written (we use Moreau's identity for the last two lines)

\begin{align}
    x^{k+1}   &= \prox_{\gamma R}(x^k - \gamma v^k - \gamma y^k)\\
    \widetilde{x^{k+1}} &= x^{k+1}\\
    y_j^{k+1} &= \prox_{\frac{1}{\gamma_j} g_j^\star}(y_j^k + \frac{1}{\gamma_j}\widetilde{x^{k+1}} ), \quad j \sim p_j.
\end{align}

We introduce the Decoupling 2 method:
% \adil{Douglas Rachford algorithm is a particular case of Decoupling and Douglas Rachford-normalized form is a particular case of Decoupling2. Since the two form of DR are equivalent, are Decoupling and Decoupling2 equivalent? I don't think so because }

\begin{align}
    x^{k+1}   &= \prox_{\gamma R}(x^k - \gamma v^k - \gamma y^k)\\
    \widetilde{x^{k+1}} &= 2x^{k+1} - x^k\\
    y_j^{k+1} &= \prox_{\frac{1}{\gamma_j} g_j^\star}(y_j^k + \frac{1}{\gamma_j}\widetilde{x^{k+1}} ), \quad j \sim p_j.
\end{align}

If $m=1$ and if $v^k$ is the true gradient, the Decoupling 2 method is Vu Condat algorithm~\cite{con-jota13}:
\begin{align}
    x^{k+1}   &= \prox_{\gamma R}(x^k - \gamma \nabla f(x^k) - \gamma y^k)\\
    y^{k+1} &= \prox_{\frac{1}{\gamma} g^\star}(y^k + \frac{1}{\gamma}(2x^{k+1}-x^k)).
\end{align}



Roadmap.
\begin{itemize}
    \item First, we introduce Vu-Condat algorithm and provide conditions under which this algorithm converges linearly
    \item Then, we replace the gradient appearing in Vu-Condat by a variance reduced unbiaise estimate. The conditions for linear convergence don't change.
    \item Finally, we do coordinate descent for Vu-Condat-VR. This method called, Decoupling 2, is similar to the Decoupling method~\cite{mishchenko2019stochastic}, with similar guarantees. 
\end{itemize}

\section{Vu-Condat}
\subsection{Forward-Backward}
Consider an Euclidean space $\sX$ (it is important to consider an abstract space and not just $\bR^d$ to simplify the presentation), two monotone operators $A,B$ and a positive definite matrix $P$.
The Forward Backward algorithm is written
\begin{equation}
    \label{eq:FB}
    x^{k+1} = x^k - \gamma P^{-1} (B(x^k) + A(x^{k+1}))
\end{equation}
Note that $x^{k+1}$ is implicitly defined. The equality $=$ is actually an inclusion $\in$ because monotone operators can be set-valued, but we assume for simplicity that $A,B$ are single valued (but it is not necessary).

The goal of the FB algorithm is to converge to a zero of $A+B$ i.e a point $x^\star$ such that $0 = A(x^\star) + B(x^\star)$. The most famous application of the FB algorithm is the case where $P=I$, $B = \nabla F$ and $A = \partial G$, where $F$ and $G$ are two convex functions, and $F$ is smooth. In this case the FB algorithm boils down to the prox-grad algorithm and converges to a minimizer of $F+G$.

If $P = I$, the analysis of the FB algorithm is starts follows:
\begin{align*}
    &\|x^{k+1} - x^\star\|^2 \\=& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k),x^k-x^\star} - 2\gamma\ps{A(x^{k+1}),x^k-x^\star} +  \gamma^2\|B(x^k) + A(x^{k+1})\|^2\\ 
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star} - 2\gamma\ps{A(x^{k+1})-A(x^\star),x^k-x^\star} +  \gamma^2\|B(x^k) + A(x^{k+1})\|^2\\
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}  +  \gamma^2\|B(x^k) + A(x^{k+1})\|^2\\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star} -2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k}-x^{k+1}}\\
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}  +  \gamma^2\|B(x^k) + A(x^{k+1})\|^2\\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star} -2\gamma^2\ps{A(x^{k+1})-A(x^\star),B(x^k)+A(x^{k+1})}\\
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}  +  \gamma^2\|B(x^k)\|^2 - \gamma^2\|A(x^{k+1})\|^2 \\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star} +2\gamma^2\ps{A(x^\star),B(x^k)+A(x^{k+1})}.
\end{align*}
The simplifications is the last line comes from expanding the square. Now we use 
$$
\gamma^2\|B(x^k)\|^2 - \gamma^2\|A(x^{k+1})\|^2 +2\gamma^2\ps{A(x^\star),B(x^k)+A(x^{k+1})} = -\gamma^2\|A(x^{k+1}) - A(x^\star)\|^2 + \gamma^2 \|B(x^k) + A(x^\star) \|^2,
$$
To get 
\begin{align*}
    &\|x^{k+1} - x^\star\|^2 \\
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}  -\gamma^2\|A(x^{k+1}) - A(x^\star)\|^2 + \gamma^2 \|B(x^k) + A(x^\star) \|^2\\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star}.
\end{align*}
Finally, we use
$$B(x^k) + A(x^\star) = B(x^k) - B(x^\star) + B(x^\star) + A(x^\star) = B(x^k) - B(x^\star)$$
and we have 
\begin{align}
    \label{eq:FB-final}
    &\|x^{k+1} - x^\star\|^2 \\
    =& \|x^k - x^\star\|^2 -\gamma^2\|A(x^{k+1}) - A(x^\star)\|^2 + \gamma^2 \|B(x^k) - B(x^\star) \|^2\\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star} - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}.
\end{align}
The two inner product are negative because of monotonicity. The only nonnegative term is $\|B(x^k) - B(x^\star) \|^2$. It is managed using some smoothness condition. To get a linear rate, we need a contraction. This contraction can come from the three negative terms. For example, if $A$ or $B$ is strongly monotone we get a contraction. But this is not the only case where we have a contraction, we can use some partial strong monotonicity (we will see an example later).

The analysis in the case $P \neq I$ follows the same lines, by replacing $\|\cdot\|$ (resp. $\ps{\cdot,\cdot}$) by the norm (resp. the inner product) induced by $P$. 




\subsection{Vu-Condat as instance of Forward Backward}

Vu-Condat algorithm aims at minimizing $\min F(x) + G(x) + H(Mx)$ where $F,G,H$ are convex functions, $F$ is smooth and $M : \sX \to \sY$ is a matrix. It is a splitting algorithm: the user is only required to compute $\nabla F$, $\prox_G$ and $\prox_H$ separately. It is a primal dual algorithm that operates in the primal dual space $\sX \times \sY$. In other words, the iterates $(x^k,y^k)$ are elements of $\sX \times \sY$. If $F=0$, Vu-Condat boils down to Chambolle-Pock algorithm.

The iterations of Vu Condat are written
\begin{align}
    x^{k+1}   &= \prox_{\gamma G}(x^k - \gamma \nabla F(x^k) - \gamma M^T y^k)\\
    y^{k+1} &= \prox_{\frac{1}{\gamma} H^\star}(y^k + \frac{1}{\gamma}M(2x^{k+1}-x^k)).
\end{align}

Interestingly, Vu-Condat can be obtained as an instance of the FB algorithm by carefully instanciating $A,B,P$. In the case of Vu-Condat, the optimization space is $\sX \times \sY$ i.e the iterates are elements $(x,y) \in \sX \times \sY$. The operators are defined on $\sX \times \sY$ by
\[
P_\gamma = \begin{bmatrix} I &  -\gamma M^T \\ -\gamma M & I \end{bmatrix},
\]
\[
A(x,y) = \begin{bmatrix} \partial G(x)&  + M^T y \\ -M x& +\partial H^\star(y)\end{bmatrix} .
\]
\[
B(x,y) = \begin{bmatrix} \nabla F(x) \\ 0 \end{bmatrix} .
\]
To see that $A,B$ are monotone, note that $B$ is actually the gradient of the convex function $(x,y) \mapsto F(x)$ and that $A$ is the sum of a skew symmetrix matrix $L$ and the subdifferential of $(x,y) \mapsto G(x) + H^\star(y)$. Recall that a skew symmetric matrix is a monotone operator (since $\ps{Lz,z} = -\ps{Lz,z}$ we have $\ps{Lz,z} = 0 \geq 0$). As a sum, $A$ is monotone.

Note also that a zero $(x^\star,y^\star)$ of $A+B$ is a primal-dual optimal point, i.e a saddle point of the Lagragien. Therefore $x^\star$ is a solution of our problem: if $0 = A(x^\star) + B(x^\star)$ then $0 = -Mx^\star +\partial H^\star(y^\star)$ and therefore $y^\star = \partial H(Mx^\star)$. Moreover $0 = \nabla F(x^\star) + \partial G(x^\star) + M^Ty^\star = \nabla F(x^\star) + \partial G(x^\star) + M^T \partial H(Mx^\star)$, therefore $x^\star$ is a minimizer of $F+G+H \circ M$.

\subsection{Conditions under whcich Vu Condat converges linearly}
The sum of the negative term in the analysis of Forward Backward is (see Eq.~\eqref{eq:FB-final})
$$
\chi^{k+1} = -\gamma^2\|A(z^{k+1}) - A(z^\star)\|^2 - 2\gamma\ps{A(z^{k+1})-A(z^\star),z^{k+1}-z^\star} - 2\gamma\ps{B(z^k)-B(z^\star),z^k-z^\star},
$$
In the case of the monotone operators of Vu Condat, we have
(recall that the norm is the norm on the product space $\sX \times \sY$, and we denote $z^k = (x^k,y^k)$ and $z^\star = (x^\star,y^\star)$)
    \begin{itemize}
        \item $\ps{A(z^{k+1})-A(z^\star),x^{k+1}-x^\star} = \ps{\partial G(x^{k+1})-\partial G(x^\star),x^{k+1}-x^\star}_\sX + \ps{\partial H^\star(y^{k+1})-\partial H^\star(y^\star),y^{k+1}-y^\star}_\sY$
        \item $\ps{B(z^k)-B(z^\star),z^k-z^\star} = \ps{\nabla F(x^k) - \nabla F(x_\star),x-x^\star}_\sX$
        \item $\|A(z^{k+1}) - A(z^\star)\|^2 = \|\partial G(x^{k+1}) + M^T y^{k+1} - (\partial G(x^{\star}) + M^T y^{\star})\|_\sX^2 + \|(-M x^{k+1} + \partial H^\star(y^{k+1})) - (-M x^{\star} + \partial H^\star(y^{\star}))\|_\sY^2$
    \end{itemize}

To have linear rate we need contraction in $x$ and $y$. Each of these term can bring a contraction in $x$ or $y$. 

Here we give some sufficient conditions. We have linear rate if 
    \begin{enumerate}
        \item \label{item:smooth} $F$ strongly convex (or $G$ strongly convex) and $H^\star$ strongly convex (i.e $H$ smooth)
        \item \label{item:lin}$F$ strongly convex (or $G$ strongly convex) and $M^T = I$ (more generally, $M^T \geq \omega I$ \asnote{I mean $\|M^Tx\| \geq \omega\|x\|$ for some $\omega$ which is actually equivalent to $M^T$ injective or $M$ surjective}) and $G = 0$ (more generally, $G$ smooth)
        \item \dots
        \item \dots
        \item \dots
        \item \dots\asnote{Complete}
    \end{enumerate}


    \section{Vu-Condat with VR}
    In Eq.~\eqref{eq:FB-final} we replace the monotone operators $A,B$ by their values in the case of Vu-Condat algorithm.  In this particular case,~\eqref{eq:FB-final} boils down to:
\begin{align*}
    \|x^{k+1} - x^\star\|_\sX^2 + \|y^{k+1} - y^\star\|_\sX^2 
    =& \|x^k - x^\star\|_\sX^2 + \|y^k - y^\star\|_\sY^2 \\
    &-\gamma^2\|\partial G(x^{k+1}) + M^T y^{k+1} - (\partial G(x^{\star}) + M^T y^{\star})\|_\sX^2 \\
    &- \gamma^2\|(-M x^{k+1} + \partial H^\star(y^{k+1})) - (-M x^{\star} + \partial H^\star(y^{\star}))\|_\sY^2 \\
    &+ \gamma^2 \|\nabla F(x^k) - \nabla F(x_\star) \|_\sX^2\\
    &- 2\gamma\ps{\partial G(x^{k+1})-\partial G(x^\star),x^{k+1}-x^\star}_\sX \\
    &- 2\gamma\ps{\partial H^\star(y^{k+1})-\partial H^\star(y^\star),y^{k+1}-y^\star}_\sY \\
    &- 2\gamma\ps{\nabla F(x^k) - \nabla F(x_\star),x-x^\star}_\sX.
\end{align*}
Denote 
\begin{align*}
    \chi^{k+1} 
    =& \gamma\|\partial G(x^{k+1}) + M^T y^{k+1} - (\partial G(x^{\star}) + M^T y^{\star})\|_\sX^2 \\
    & +\gamma\|(-M x^{k+1} + \partial H^\star(y^{k+1})) - (-M x^{\star} + \partial H^\star(y^{\star}))\|_\sY^2 \\
    &+ 2\ps{\partial G(x^{k+1})-\partial G(x^\star),x^{k+1}-x^\star}_\sX \\
    &+ 2\ps{\partial H^\star(y^{k+1})-\partial H^\star(y^\star),y^{k+1}-y^\star}_\sY.
\end{align*}
Assume from now on the every gradient $\nabla F(x^k)$ is replaced by an unbiased estimate $g^k$. We have


\begin{align*}
    \|x^{k+1} - x^\star\|_\sX^2 + \|y^{k+1} - y^\star\|_\sX^2 
    =& \|x^k - x^\star\|_\sX^2 + \|y^k - y^\star\|_\sY^2 \\
    &+ \gamma^2 \|g^k - \nabla F(x_\star) \|_\sX^2\\
    &- 2\gamma\ps{g^k - \nabla F(x_\star),x-x^\star}_\sX\\
    &-\gamma \chi^{k+1}.
\end{align*}

Moreover, we assume that $g^k$ is variance reduced in the sense of~\cite{gorbunov2019unified}: there exist an adapted stochastic process $(\sigma_k)_k$ and constants $A,B,C \geq 0$ such that
\begin{align}
    \bE_k(\|g^k - \nabla f(x^\star)\|^2) &\leq 2AD_F(x^k,x^\star) + B\sigma_k\\
    \bE_k(\sigma_{k+1}^2) &\leq (1-\rho)\sigma_k^2 + 2CD_F(x^k,x^\star),
\end{align}
where $\bE_k$ denotes the conditional expectation.


We can mimic the proof of Theorem 1 in~\cite{gorbunov2019unified} to get a contraction for $\|x^k-x^\star\|^2$.
Taking conditional expectation and using $\mu$-quasi strong convexity we have

\begin{align*}
    \bE_k \|x^{k+1} - x^\star\|_\sX^2 + \|y^{k+1} - y^\star\|_\sX^2 
    \leq& (1-\gamma\mu)\|x^k - x^\star\|_\sX^2 + \|y^k - y^\star\|_\sY^2 \\
    &+ \gamma^2 (2AD_F(x^k,x^\star) + B\sigma_k) - 2\gamma D_F(x^k,x^\star)\\
    &-\gamma\chi^{k+1}.
\end{align*}

Therefore, if $M>0$ is large enough in order to have $1-\rho+B/M < 1$,

\begin{align*}
    &\bE_k \|x^{k+1} - x^\star\|_\sX^2 + \|y^{k+1} - y^\star\|_\sX^2 + M\gamma^2\bE(\sigma_{k+1}^2)\\
    \leq& (1-\gamma\mu)\|x^k - x^\star\|_\sX^2 + \|y^k - y^\star\|_\sY^2 -2\gamma(1-A\gamma)D_F(x^k,x^\star) + B\gamma^2\bE(\sigma_k^2)\\
     &+ M\gamma^2(1-\rho)\bE(\sigma_k^2)+ 2M\gamma^2CD_F(x^k,x^\star)\\
    &-\gamma\chi^{k+1}\\
    =& (1-\gamma\mu)\|x^k - x^\star\|_\sX^2 + \|y^k - y^\star\|_\sY^2 -2\gamma(1-\gamma(A+MC))D_F(x^k,x^\star)\\
     &+ M\gamma^2(1-\rho+\frac{B}{M})\bE(\sigma_k^2)\\
    &-\gamma\chi^{k+1}.
\end{align*}
Finally, denoting $\alpha = \max(1-\rho+B/M,1-\gamma\mu) < 1$ and $W^k = \|x^k-x^\star\|^2 + M\gamma^2\sigma^k$ we have

\asnote{Weakly convex case?}

\begin{align*}
    &\bE W^{k+1} + \bE \|y^{k+1} - y^\star\|_\sX^2\\
    \leq& \alpha W^k + \bE\|y^k - y^\star\|_\sY^2 -2\gamma(1-\gamma(A+MC))D_F(x^k,x^\star)\\
     &-\gamma\chi^{k+1}.
\end{align*}

Now, one can use $\chi^k$ to also have a contraction in $\bE\|y^k - y^\star\|_\sY^2$. \cite{Put expectation everywhere}

\section{An instance of Vu Condat}

Consider the problem 
\begin{equation}
    \label{eq:appli}
\min_{x \in \sX} f(x) + \sum_{j=1}^m g_j(x) + R(x)
\end{equation}
This problem can be tackled by Vu-Condat by setting $F(x) = f(x)$, $\sY = \sX^m$, $H(y_1,\ldots,y_m) = \sum_j g_j(y_j)$ and $M : \sX \to \sY, Mx = 1/m (x,\ldots,x)$ and $G(x) = R(x)$.
The iterates are now elements $(x,(y_1,\ldots,y_m)) \in \sX \times \sX^m$. Note that $G$ is separable and $M^T(y_1,\ldots,y_m) = \frac{1}{m}\sum_{j=1}^m y_j = \overline{y}$. The prox of $G$ can be computed in parallel, therefore Vu-Condat applied to~\ref{eq:appli} is written\asnote{Check the step sizes. Rewrite decoupling as Vu-Condat}
    \begin{align}
        x^{n+1}   &= \prox_{\gamma R}(x^n - \gamma v^n - \gamma \frac{1}{m}\sum_{j=1}^m y_j^n)\\
        \widetilde{x^{n+1}} &= 2x^{n+1} - x^n\\
        y_j^{n+1} &= \prox_{\frac{1}{\gamma_j} g_j^\star}(y_j^n + \frac{1}{\gamma_j}\widetilde{x^{n+1}} ), \forall j.
    \end{align}
    
The value of $\chi^{k+1}$ for this instance of Vu-Condat is 
\begin{align*}
    \chi^{k+1} 
    =& \gamma\|A(x^{k+1}) + \overline{y^{k+1}} - (A(x^{\star}) + \overline{y^{\star}})\|_\sX^2 \\
    & +\sum_{j=1}^m \gamma  \|(-\frac{1}{m}x^{k+1} + \partial g_j^\star(y_j^{k+1})) - (-x^{\star} + \partial g_j^\star(y_j^{\star}))\|_\sX^2 \\
    &+ 2\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star}_\sX \\
    &+ 2 \sum_j \ps{\partial g_j^\star(y_j^{k+1})-\partial g_j^\star(y_j^\star),y_j^{k+1}-y_j^\star}_\sX.
\end{align*}

\begin{itemize}
    \item If $F$ is strongly convex we have a contraction in $x$. If moreover, $g_j$ is smooth for every $j$, we have a contraction in $y_j$ (because in this case $g_j^\star$ is strongly convex). This si the case in Th 6 of\cite{mishchenko2019stochastic}.
    \item If $R = 0$ (or smooth) and if there exists orthogonal\asnote{Supplementaire sufficient? All what we need is linear independence of $(y_j)_j$} vector subspaces $S_1,\ldots,S_m$ of $\sX$ such that for every $k$, $y_j^k \in S_j$, then the first term in $\chi^{k+1}$ is equal to $\frac{1}{m^2}\|y^{k+1} - y^\star\|_{\sX^m}^2$ and we have contraction in $y$. This is the case in Th 4 and 5 of~\cite{mishchenko2019stochastic}\asnote{Not exactly but this is the idea. Precise in order to extend}
\end{itemize}
We now add coordinate descent on top of this method in order to obtain the Decoupling method. The conditions under which linear rate holds don't change.



\section{Coordinate descent}


\section{Possible extensions}

More general linear operator $M$

\subsubsection*{References}
\renewcommand\refname{\vskip -1cm}
\bibliographystyle{apalike}
\bibliography{math}

\end{document}
