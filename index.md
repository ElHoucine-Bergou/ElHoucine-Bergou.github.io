---
header:
  overlay_image: unsplash-gallery-image-2.jpg
---


I'm a postdoctoral fellow at KAUST, Saudi Arabia, hosted by [Peter Richtarik](https://richtarik.org/). I did my PhD at Telecom ParisTech, France, under the supervision of [Pascal Bianchi](https://bianchi.wp.imt.fr/) and [Walid Hachem](http://www-syscom.univ-mlv.fr/~whachem/) in the S2A (Signal, Statistics and Learning) team. 

I use optimization, statistics, optimal transport, convex analysis etc. to study machine learning algorithms. I received the Masters degrees in 2015 from ENSAE ParisTech, where I studied statistics, and from Paris XI University, where I studied probability theory. Here is my CV ([english]({{site.baseurl}}{% link postdoc_en.pdf %}) or [french]({{site.baseurl}}{% link postdoc_fr.pdf %})).

## News

- The sessions I am co-organizing at [SIAM Conference on Optimization 2020](https://www.siam.org/conferences/cm/conference/op20) are postponed until further notice, due to Covid19 outbreak. 
I am co-organizing the mini-symposium "Gradient flows and interactions between optimization and sampling algorithms" (Part I and Part II) with [Anna Korba](https://akorba.github.io/) and the mini-symposium "Recent advances in primal-dual splitting for convex optimization" with [Laurent Condat](https://lcondat.github.io/).

- I will give a talk at the [Second Symposium on Machine Learning and Dynamical Systems](https://sites.google.com/site/boumedienehamzi/second-symposium-on-machine-learning-and-dynamical-systems) which will be virtually held at the Fields Institute, Toronto. 


## Selected papers

- D. Kovalev, A. Salim and P. Richt치rik, "[Optimal and Practical Algorithms for Smooth and Strongly Convex Decentralized Optimization](https://arxiv.org/abs/2006.09270)", submitted, May 2020.

- A. Korba, A. Salim, M. Arbel, G. Luise and A. Gretton, "[A Non-Asymptotic Analysis for Stein Variational Gradient Descent](https://arxiv.org/abs/2006.09797)", submitted, May 2020. 

- A. Salim and P. Richt치rik, "[Primal Dual Interpretation of the Proximal Stochastic Gradient Langevin Algorithm](https://arxiv.org/abs/2006.09270)", submitted, May 2020. 

- A. Salim, L. Condat, K. Mishchenko and P. Richt치rik, "[Dualize, Split, Randomize: Fast Nonsmooth Optimization Algorithms](https://arxiv.org/abs/2004.02635)", submitted, April 2020. 

- A. Salim, A. Korba and G. Luise, "[The Wasserstein Proximal Gradient Algorithm](https://arxiv.org/abs/2002.03035)", submitted, January 2020. 

- A. Salim, [A Strong Law of Large Numbers for Random Monotone Operators](https://arxiv.org/abs/1910.04405), Octobre 2019. 

- P. Bianchi, W. Hachem and A. Salim, [A Fully Stochastic Primal-Dual Algorithm]({{site.baseurl}}{% link Research/pridu19.pdf %}), submitted, January 2019. 

- A. Salim, D. Kovalev and P. Richt치rik, "[Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates]({{site.baseurl}}{% link Research/langevin19.pdf %})", Neurips 2019, Vancouver, Canada, **Spotlight**. (Here is the slides of the [Spotlight]({{site.baseurl}}{% link Research/spla-slides.pdf %}), the [Code](https://github.com/adil-salim/SPLA) and the [Poster]({{site.baseurl}}{% link Research/poster-langevin19.pdf %})).

- M. Arbel, A. Korba, A. Salim and A. Gretton, "[Maximum Mean Discrepancy Gradient Flow]({{site.baseurl}}{% link Research/mmd19.pdf %})", Neurips 2019, Vancouver, Canada.

- A. Salim, P. Bianchi, and W. Hachem, [Snake: a Stochastic Proximal Gradient Algorithm for Regularized Problems over Large Graphs]({{site.baseurl}}{% link Research/snake18.pdf %}), _Transaction on Automatic Control_, March 2018.

- P. Bianchi, W. Hachem, and A. Salim, [A constant step Forward-Backward algorithm involving random maximal monotone operators]({{site.baseurl}}{% link Research/joca1813_revised.pdf %}), _Journal of Convex Analysis_, March 2018.
