
\documentclass{article}
\usepackage{nips_2018,graphicx}

 \usepackage[english]{babel} 
  \usepackage{times}		
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} 
\DeclareUnicodeCharacter{00A0}{~}
\usepackage{amssymb,amsmath,amscd,amsfonts,amsthm,bbm,mathrsfs,yhmath}
\usepackage[shortlabels]{enumitem}
\usepackage{hyperref}

\usepackage[textwidth=2cm, textsize=footnotesize]{todonotes}  
\setlength{\marginparwidth}{1.5cm}               %  this goes with todonotes
\newcommand{\aknote}[1]{\todo[color=cyan!20]{#1}}
\newcommand{\asnote}[1]{\todo[color=green!]{#1}}

\newcommand{\cF}{{\mathcal F}} 
\newcommand{\cS}{{\mathcal S}} 
\newcommand{\cM}{{\mathcal M}} 
\newcommand{\cP}{{\mathcal P}} 

\DeclareMathOperator{\prox}{prox}


\newcommand{\cC}{{\mathcal C}} 
\newcommand{\E}{{{\mathbb E}}} 
\newcommand{\bR}{{\mathbb R}} 
\newcommand{\bP}{{\mathbb P}} 
\newcommand{\bE}{{\mathbb E}} 
\newcommand{\sX}{{\mathsf X}} 
\newcommand{\sY}{{\mathsf Y}} 

\newcommand{\cL}{{{\mathcal L}}}



\newcommand{\KL}{\mathop{\mathrm{KL}}\nolimits}
\newcommand{\tr}{\mathop{\mathrm{tr}}\nolimits}
\newcommand{\ps}[1]{\langle #1 \rangle}
\newcommand{\Supp}{\mathop{\mathrm{Supp}}\nolimits}


\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{condition}{Condition}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}

\title{From $\sigma_k$ to Decoupling}


\begin{document}

\maketitle

\begin{abstract} 

\end{abstract}

Generally, the goal of this work is to go from the analysis of SGD proposed in the $\sigma_k$ paper~\cite{gorbunov2019unified} to the decoupling paper~\cite{mishchenko2019stochastic}.

In~\cite{gorbunov2019unified}, SGD is analyzed under the following assumptions 
\begin{assumption}
Unbiasedness
\begin{equation}
    \bE[g^k|x^k] = \nabla f(x^k)
\end{equation}
\end{assumption}

We are especially trying to understand the following points.
\begin{itemize}
    \item What is the fundamental reasons behind the linear rates obtained in~\cite{mishchenko2019stochastic} (Start with the case where $m=1$)
    \item \cite{gorbunov2019unified} provide a tight analysis of SGD, can we generalize it to the setting of~\cite{mishchenko2019stochastic}
    \item If we particularize~\cite{mishchenko2019stochastic} to the setting of SGD, what kind of analysis do we get? Especially, it allows biased estimate of the gradient.
    \item In~\cite{mishchenko2019stochastic}, what is the assumption made on $v_t$ ? what is $Y_t$?
    \item Minibatch analysis of~\cite{mishchenko2019stochastic} 
\end{itemize}
\section{About Decoupling}

We shall provide an approach to understand some results of the Decoupling paper. We proceed as follows:
\begin{itemize}
    \item First, we introduce Vu-Condat algorithm and provide conditions unde which this algorithm converges linearly (these conditions should be already known in the literature but we have to check)
    \item Second, we consider an instance of Vu-Condat algorithm that we call the Original method. As an instance of Vu-Condat, we can provide conditions under which the Original method converges linearly
    \item Third, we add variance reduction for every gradient appearing in the Original method. This leads to the VR-Original method. This doesn't change the conditions under which we have linear rate (the proof is an extension of the proof of the main theorem of the $\sigma_k$ paper)
    \item Finally, we add "coordinate descent" on top of VR-Original method. This leads to the Decoupling method. This doesn't change the conditions under which we have linear rate (This is based on standard considerations regarding coordinate descent).
\end{itemize}

\section{Vu-Condat}

Consider an Euclidean space $\sX$ (it is important to consider an abstract space and not just $\bR^d$ to simplify the presentation), two monotone operators $A,B$ and a positive definite matrix $P$.
The Forward Backward algorithm is written
\begin{equation}
    \label{eq:FB}
    x^{k+1} = x^k - \gamma P^{-1} (B(x^k) + A(x^{k+1}))
\end{equation}
Note that $x^{k+1}$ is implicitly define. The equality $=$ is actually an inclusion $\in$ because monotone operators can be multi-valued, but we assume that our monotone operators are single valued to keep it simpler (but it is not necessary)
The goal of the FB algorithm is to converge to a zero of $A+B$ i.e a point $x_\star$ such that $0 = A(x_\star) + B(x_\star)$. The most famous application of the FB algorithm is the case where $P=I$, $B = \nabla F$ and $A = \partial G$, where $F$ and $G$ are two convex functions. In this case the FB algorithm boils down to the prox-grad algorithm and converges to a minimizer of $F+G$.

If $P = I$, the analysis of the FB algorithm is as follows:
\begin{align*}
    &\|x^{k+1} - x^\star\|^2 \\=& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k),x^k-x^\star} - 2\gamma\ps{A(x^{k+1}),x^k-x^\star} +  \gamma^2\|B(x^k) + A(x^{k+1})\|^2\\ 
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star} - 2\gamma\ps{A(x^{k+1})-A(x^\star),x^k-x^\star} +  \gamma^2\|B(x^k) + A(x^{k+1})\|^2\\
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}  +  \gamma^2\|B(x^k) + A(x^{k+1})\|^2\\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star} -2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k}-x^{k+1}}\\
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}  +  \gamma^2\|B(x^k) + A(x^{k+1})\|^2\\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star} -2\gamma^2\ps{A(x^{k+1})-A(x^\star),B(x^k)+A(x^{k+1})}\\
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}  +  \gamma^2\|B(x^k)\|^2 - \gamma^2\|A(x^{k+1})\|^2 \\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star} +2\gamma^2\ps{A(x^\star),B(x^k)+A(x^{k+1})}.
\end{align*}
The simplifications is the last line comes from expanding the square. Now we use 
$$
\gamma^2\|B(x^k)\|^2 - \gamma^2\|A(x^{k+1})\|^2 +2\gamma^2\ps{A(x^\star),B(x^k)+A(x^{k+1})} = -\gamma^2\|A(x^{k+1}) - A(x^\star)\|^2 + \gamma^2 \|B(x^k) + A(x^\star) \|^2,
$$
To get 
\begin{align*}
    &\|x^{k+1} - x^\star\|^2 \\
    =& \|x^k - x^\star\|^2 - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}  -\gamma^2\|A(x^{k+1}) - A(x^\star)\|^2 + \gamma^2 \|B(x^k) + A(x^\star) \|^2\\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star}.
\end{align*}
Finally, we use
$$B(x^k) + A(x^\star) = B(x^k) - B(x^\star) + B(x^\star) + A(x^\star) = B(x^k) - B(x^\star)$$
and we have 
\begin{align}
    \label{eq:FB-final}
    &\|x^{k+1} - x^\star\|^2 \\
    =& \|x^k - x^\star\|^2 -\gamma^2\|A(x^{k+1}) - A(x^\star)\|^2 + \gamma^2 \|B(x^k) - B(x^\star) \|^2\\
    &- 2\gamma\ps{A(x^{k+1})-A(x^\star),x^{k+1}-x^\star} - 2\gamma\ps{B(x^k)-B(x^\star),x^k-x^\star}.
\end{align}
The two inner product are negative because of monotonicity. The only nonnegative term is $\|B(x^k) - B(x^\star) \|^2$. It is managed using some smoothness condition (namely, cocoercivity or lipschitzness). To get a linear rate, we need a contraction. This contraction can come from the three negative terms. For example, if $A$ or $B$ is strongly monotone we get a contraction. But this is not the only case where we have a contraction, we can use some partial strong monotonicity (we will see an example later).

The analysis in the case $P \neq I$ follows the same lines, by replacing $\|\cdot\|$ (resp. $\ps{\cdot,\cdot}$) by the norm (resp. the inner product) induced by $P$. 





Vu-Condat algorithm aims at minimizing $\min F(x) + G(x) + H(Mx)$ where $F,G,H$ are convex functions, $F$ is smooth and $M : \sX \to \sY$ is a matrix. It is a splitting algorithm: he user is only require to compute $\nabla F$, $\prox_G$ and $\prox_H$ separately. It is a primal dual algorithm. If $F=0$ is boils down to Chambolle-Pock algorithm.

Interestingly, Vu-Condat can be obtained as an instance of the FB algorithm by carefully instanciating $A,B,P$. This is the easiest way to understand Vu-Condat algorithm. In the case of Vu-Condat, the optimization space is $\sX \times \sY$ i.e the iterates are elements $(x,y) \in \sX \times \sY$. The operators are defined on $\sX \times \sY$ by
\[
P_\gamma = \begin{bmatrix} I &  -\gamma M^T \\ -\gamma M & I \end{bmatrix},
\]
\[
A(x,y) = \begin{bmatrix} \partial G(x)&  + M^T y \\ -M x& +\partial H^\star(y)\end{bmatrix} .
\]
\[
B(x,y) = \begin{bmatrix} \nabla F(x) \\ 0 \end{bmatrix} .
\]
To see that $A,B$ are monotone, note that $B$ is actually the gradient of the function $(x,y) \mapsto F(x)$ and that $A$ is the sum of a skew symmetrix matrix $M$ and the subdifferential of $(x,y) \mapsto G(x) + H^\star(y)$. Recall that a skew symmetric matrix is a monotone operator (since $\ps{Mz,z} = -\ps{Mz,z}$ we have $\ps{Mz,z} \geq 0$). As a sum, $A$ is monotone.

Note also that a zero $(x^\star,y^\star)$ of $A+B$ is a primal-dual optimal point, i.e a saddle point of the Lagragien. Therefore $x^\star$ is a solution of our problem: since $0 = -Mx +\partial H^\star(y)$ we have $y = \partial H(Mx)$ and therefore $0=  \nabla F(x^\star) + \partial G(x^\star) + \partial H(M x^\star)$.

\subsection{Conditions under whcich Vu Condat converges linearly}
The negative terms in the analysis of Vu-Condat are the following (recall that the norm is the norm on the product space $\sX \times \sY$)
    \begin{itemize}
        \item $\ps{A(z^{k+1})-A(z^\star),x^{k+1}-x^\star} = \ps{\partial G(x^{k+1})-\partial G(x^\star),x^{k+1}-x^\star}_\sX + \ps{\partial H^\star(y^{k+1})-\partial H^\star(y^\star),y^{k+1}-y^\star}_\sY$
        \item $\ps{B(z^k)-B(z^\star),z^k-z^\star} = \ps{\nabla F(x) - \nabla F(x_\star),x-x^\star}_\sX$
        \item $\|A(z^{k+1}) - A(z^\star)\|^2 = \|\partial G(x^{k+1}) + M^T y^{k+1} - (\partial G(x^{\star}) + M^T y^{\star})\|_\sX^2 + \|(-M x^{k+1} + \partial H^\star(y^{k+1})) - (-M x^{\star} + \partial H^\star(y^{\star}))\|_\sY^2$
    \end{itemize}

To have linear rate we need contraction in $x$ and $y$. Each of these term can bring a contraction. 

Here we give some sufficient conditions. We have linear rate if 
    \begin{enumerate}
        \item \label{item:smooth} $F$ strongly convex (or $G$ strongly convex) and $H^\star$ strongly convex (i.e $H$ smooth)
        \item \label{item:lin}$F$ strongly convex (or $G$ strongly convex) and $M^T = I$ (more generally, $M^T \geq \omega I$ \asnote{I mean $\|Mx\| \geq \omega\|x\|$ for some $\omega$ which is actually equivalent to $M^T$ injective}) and $G = 0$ (more generally, $G$ smooth)
        \item \dots
        \item \dots
        \item \dots
        \item \dots\asnote{Complete}
    \end{enumerate}

\section{An instance of Vu Condat}

Consider the problem 
\begin{equation}
    \label{eq:appli}
\min_{x \in \sX} f(x) + \sum_{j=1}^m g_j(x)
\end{equation}
This problem can be tackled by Vu-Condat by setting $F(x) = f(x)$, $\sY = \sX^m$, $G(y_1,\ldots,y_m) = \sum_j g_j(y_j)$ and $M : \sX \to \sY, Mx = 1/m (x,\ldots,x)$
The iterates are now elements $(x,(y_1,\ldots,y_m)) \in \sX \times \sX^m$. Note that $G$ is separable and $M^T(y_1,\ldots,y_m) = \frac{1}{m}\sum_{j=1}^m y_j$. The prox of $G$ can be computed in parallel, therefore Vu-Condat applied to~\ref{eq:appli} is written\asnote{Check the step sizes. Rewrite decoupling as Vu-Condat}
\begin{align*}
   ....
\end{align*}

We call this algorithm the Original method. It is a parallel version of the decoupling algorithm without VR.
Conditions under which this algorithm converges linearly can be borrowed from the previous section.

We now add VR and coordinate descent to the Original method in order to obtain the Decoupling method. The conditions under which linear rate holds don't change.

\section{VR}

\section{Coordinate descent}


\section{Something else: Stochastic Chambolle-Pock algorithm}
% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------

We want to solve 
\begin{equation}
    \min_x \bE_\xi(g(x,\xi)) \quad\text{s.t.}\quad \bE(L)x = \bE(b),
\end{equation}
where $L$ is a random matrix and $b$ a random vector. We start with $b$ and $L$ deterministic. 

Chambolle Pock for this problem is written
\begin{align}
    x_{n+1} &= \prox_{\gamma g(\cdot,\xi_{n+1})}(x_n - \gamma L_{n+1}^T y_n)\\
    y_{n+1} &= y_n + \gamma L_{n+1}(2 x_{n+1} - x_n).
\end{align}
Let $z_n = (x_n, y_n)$, 
\[
P_\gamma = \begin{bmatrix} I &  -\gamma L^T \\ -\gamma L & I \end{bmatrix},
\]
\[
A(z,\xi) = \begin{bmatrix} \partial f(x,\xi)&  + L^T y \\ -L x& \end{bmatrix} .
\]

Algorithm:

$$
P_\gamma(z_{n+1} - z_n) \in -\gamma A(z_{n+1}) 
$$


\begin{align*}
&\|z_{n+1} - z_\star\|_\gamma^2 \\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 + \ps{z_n - z_\star,(P_{n+1} - P_n)z_n - z_\star}\\
& -2\gamma \ps{A(z_{n+1}) - \varphi,z_{n+1} - z_\star} -2\gamma \ps{\varphi,z_{n} - z_\star} -2\gamma \ps{\varphi,z_{n+1} - z_n}\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 +  \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C\\
& -2\gamma \alpha \ps{A(z_{n+1}) - \varphi,z_{n+1} - z_\star} \\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 +  \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C\\
& -2\gamma\alpha \left(( g(x_{n+1},\xi_{n+1}) + \ps{L x_{n+1} - b,y_\star} ) - ( g(x_\star,\xi_{n+1}) + \ps{L x_\star - b,y_{n+1}} )\right)\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 +  \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C\\
& -2\gamma\alpha \left(( g_\gamma(x_{n} - \gamma L^T y_{n},\xi_{n+1}) + \ps{L x_{n}-b,y_\star} ) - ( g(x_\star,\xi_{n+1}) + \ps{L x_\star-b,y_{n}} )\right)\\
& -2\gamma\alpha \left(-\frac{1}{2\gamma}\|x_{n+1} - x_n - \gamma L^T y_n\|^2 + \ps{L (x_{n+1} - x_n),y_\star} - \ps{L x_\star - b,y_{n+1} - y_n}\right)\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 + \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C \\
& -2\gamma\alpha \left(( g_\gamma(x_{n} - \gamma L^T y_{n},\xi_{n+1}) + \ps{L x_{n}-b,y_\star} ) - ( g(x_\star,\xi_{n+1}) + \ps{L x_\star-b,y_{n}} )\right)\\
& + \alpha\|x_{n+1} - x_n - \gamma L^T y_n\|^2.\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 + \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C \\
& -2\gamma\alpha \left( g_\gamma(x_{n},\xi_{n+1}) + \ps{\nabla^\gamma g(x_n-\gamma L^T y_n,\xi_{n+1}), -\gamma L^T y_n} - \frac{\gamma}{2}\|L^T y_n\|^2 + \ps{L x_{n}-b,y_\star} \right) \\
& +2\gamma \alpha \left( g(x_\star,\xi_{n+1}) + \ps{L x_\star -b,y_{n}} \right)\\
& + \alpha\|x_{n+1} - x_n - \gamma L^T y_n\|^2\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 + \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C \\
& -2\gamma\alpha \left( g(x_{n},\xi_{n+1}) - \frac{\gamma}{2}\left(\|\nabla^0 g(x_{n},\xi_{n+1})\|^2 - \|\nabla^\gamma g(x_n-\gamma L^T y_n,\xi_{n+1})\|^2\right) + \ps{L x_{n}-b,y_\star} \right)\\
&-2\gamma\alpha \left(-\frac{\gamma}{2}\|\nabla^\gamma g(x_n-\gamma L^T y_n,\xi_{n+1})\|^2 - \gamma\ps{\nabla^\gamma g(x_n-\gamma L^T y_n,\xi_{n+1}), L^T y_n} - \frac{\gamma}{2}\|L^T y_n\|^2 \right) \\
& +2\gamma \alpha\left( g(x_\star,\xi_{n+1}) + \ps{L x_\star-b,y_{n}} \right)\\
& +\alpha \|x_{n+1} - x_n - \gamma L^T y_n\|^2\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 + \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C \\
& -2\gamma\alpha \left( g(x_{n},\xi_{n+1}) - \frac{\gamma}{2}\|\nabla^0 g(x_{n},\xi_{n+1})\|^2 + \ps{L x_{n}-b,y_\star} \right)\\
& + \alpha\|x_{n+1} - x_n\|^2 \\
& +2\gamma \alpha\left( g(x_\star,\xi_{n+1}) + \ps{L x_\star-b,y_{n}} \right)\\
& + \alpha\|x_{n+1} - x_n - \gamma L^T y_n\|^2 - \alpha\gamma^2 \|\nabla^\gamma g(x_n-\gamma L^T y_n,\xi_{n+1})\|^2\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 + \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C \\
& -2\gamma\alpha \left( g(x_{n},\xi_{n+1}) - \frac{\gamma}{2}\|\nabla^0 g(x_{n},\xi_{n+1})\|^2 + \ps{L x_{n}-b,y_\star} \right)\\
& + \alpha\|x_{n+1} - x_n\|^2 \\
& +2\gamma\alpha \left( g(x_\star,\xi_{n+1}) + \ps{L x_\star-b,y_{n}} \right)\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 + (\varepsilon + \alpha) \|z_{n+1} - z_{n}\|^2 + \gamma^2 C \\
& -2\gamma\alpha \left( g(x_{n},\xi_{n+1}) + \ps{L x_{n}-b,y_\star} \right)\\
& +2\gamma\alpha \left( g(x_\star,\xi_{n+1}) + \ps{L x_\star-b,y_{n}} \right).
\end{align*}
Taking $\varepsilon, \alpha$ enough small such that $(\varepsilon + \alpha) \|\cdot\|^2 \leq \|\cdot\|_\gamma^2$, and taking expectation we finally have

\begin{equation}
    \label{eq:final}
    \bE_n(\|z_{n+1} - z_\star\|_\gamma^2) \leq \|z_n - z_\star\|_\gamma^2 -2\gamma\alpha\left( \cL(x_n,y_\star) - \cL(x_\star,y_n)\right)+ \gamma^2 C.
\end{equation}
\asnote{To be added : domain, forward step}

In order to randomize $P$, let's look at
\begin{equation}
z_{n+1} - z_n = \begin{bmatrix} x_{n+1} - x_{n} \\ y_{n+1} - y_{n} \end{bmatrix}
 = \gamma \begin{bmatrix} - \nabla^\gamma g(x_n - \gamma L^T y_n,\xi_{n+1}) & -L^T y_n \\ L (2 x_{n+1} - x_n)& \end{bmatrix}.
\end{equation}

\begin{align}
P_{\gamma} (z_{n+1} - z_n) &= \gamma \begin{bmatrix} - \nabla^\gamma g(x_n - \gamma L^T y_n,\xi_{n+1}) & -L^T \left(y_n +\gamma L (2 x_{n+1} - x_n)\right) \\ L \left(x_{n} -\gamma \nabla^\gamma g(x_n - \gamma L^T y_n,\xi_{n+1}) - \gamma L^T y_n\right) \end{bmatrix}\\
&= \gamma \begin{bmatrix} - \nabla^\gamma g(x_n - \gamma L^T y_n,\xi_{n+1}) & -L^T y_{n+1} \\ L x_{n+1} \end{bmatrix}\\
&\in \gamma \begin{bmatrix} - \partial g(x_{n+1},\xi_{n+1}) & -L^T y_{n+1} \\ L x_{n+1} \end{bmatrix}\\
\end{align}

\begin{align}
    \ps{(z_{n+1} - z_n), P_{\gamma} (z_{n+1} - z_n)} = 
\end{align}






\asnote{Use inf convolution like Combettes}






\subsubsection*{References}
\renewcommand\refname{\vskip -1cm}
\bibliographystyle{apalike}
\bibliography{math}

\end{document}