
\documentclass{article}
\usepackage{graphicx}

 \usepackage[english]{babel}   % "babel.sty" + "french.sty"
% \usepackage[english,francais]{babel} % "babel.sty"
% \usepackage{french}                  % "french.sty"
  \usepackage{times}			% ajout times le 30 mai 2003
 
%% --------------------------------------------------------------
%% CODAGE DE POLICES ?
%% Si votre moteur Latex est francise, il est conseille
%% d'utiliser le codage de police T1 pour faciliter la césure,
%% si vous disposez de ces polices (DC/EC)
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} 
\DeclareUnicodeCharacter{00A0}{~}

%% ==============================================================
% Packages divers (mathématiques, etc.)   
\usepackage{amssymb,amsmath,amscd,amsfonts,amsthm,bbm,mathrsfs,yhmath}
%\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
% \usepackage{showkeys}
\usepackage{hyperref}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

\DeclareMathOperator{\var}{\mathbb Var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\zer}{zer}
\DeclareMathOperator{\aver}{av}
\DeclareMathOperator{\inter}{int}
\DeclareMathOperator{\relint}{ri}
\DeclareMathOperator{\epi}{epi}
\DeclareMathOperator{\graph}{gr}
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\support}{supp}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\lev}{lev}
\DeclareMathOperator{\rec}{rec}
\DeclareMathOperator{\cl}{cl}
\DeclareMathOperator{\co}{co}
\DeclareMathOperator{\clo}{\overline co}
\DeclareMathOperator{\distC}{\mathsf d}
\DeclareMathOperator*{\diag}{diag}

\newcommand{\leftnorm}{\left|\!\left|\!\left|}
\newcommand{\rightnorm}{\right|\!\right|\!\right|}

%\newcommand{\eqdef}{{\stackrel{\text{def}}{=}}} 
\newcommand{\eqdef}{:=} 

\newcommand{\1}{\mathbbm 1}
\newcommand{\bs}{\boldsymbol}

\newcommand{\itpx}{{\mathsf x}}
\newcommand{\sx}{{\mathsf x}}
\newcommand{\sy}{{\mathsf y}}
\newcommand{\sz}{{\mathsf z}}
\newcommand{\sw}{{\mathsf w}}
\newcommand{\sF}{{\mathsf F}}
\newcommand{\sH}{{\mathsf H}}


\newcommand{\ZZ}{\mathbb Z}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\bP}{{{\mathbb P}}} 
\newcommand{\bE}{{{\mathbb E}}} 
\newcommand{\bV}{{{\mathbb V}}} 
\newcommand{\bN}{{{\mathbb N}}} 

% Operators, domains, etc.  
\newcommand{\mA}{{\mathcal A}} 
\newcommand{\mB}{{\mathcal B}} 
\newcommand{\mC}{{\mathcal C}} 
\newcommand{\mD}{{\mathcal D}} 
\newcommand{\mO}{{\mathcal O}} 
\newcommand{\mU}{{\mathcal U}}
\newcommand{\mX}{{\mathcal X}}
\newcommand{\mY}{{\mathcal Y}}
\newcommand{\mZ}{{\mathcal Z}} 
\newcommand{\bmD}{\cl({\mathcal D})} 

\newcommand{\sA}{{\mathsf A}}
\newcommand{\sB}{{\mathsf B}}
\newcommand{\sJ}{{\mathsf J}}
\newcommand{\sX}{{\mathsf X}}
\newcommand{\sG}{{\mathsf G}}
\newcommand{\sY}{{\mathsf Y}}

\newcommand{\maxmon}{{\mathscr M}} 
\newcommand{\Selec}{{\mathfrak S}} 

% Sigma fields
\newcommand{\mcA}{{\mathscr A}} 
\newcommand{\mcB}{{\mathscr B}} 
\newcommand{\mcN}{{\mathscr N}} 
\newcommand{\mcT}{{\mathscr T}} 
\newcommand{\mcI}{{\mathscr I}} 
\newcommand{\mcF}{{\mathscr F}} 
\newcommand{\mcG}{{\mathscr G}} 
\newcommand{\mcX}{{\mathscr X}} 

\newcommand{\cP}{{{\mathcal P}}} 
\newcommand{\cS}{{{\mathcal S}}} 
\newcommand{\cZ}{{{\mathcal Z}}} 
\newcommand{\cG}{{{\mathcal G}}} 
\newcommand{\cM}{{{\mathcal M}}} 
\newcommand{\cD}{{{\mathcal D}}} 
\newcommand{\cE}{{{\mathcal E}}} 
\newcommand{\cH}{{{\mathcal H}}} 
\newcommand{\cL}{{{\mathcal L}}}
\newcommand{\cA}{{{\mathcal A}}}
\newcommand{\cT}{{{\mathcal T}}} 
\newcommand{\cN}{{{\mathcal N}}} 
\newcommand{\cK}{{{\mathcal K}}} 
\newcommand{\cI}{{{\mathcal I}}} 

% Spaces 
\newcommand{\Hil}{E}                % Hilbert   
\newcommand{\Ban}{E}                % Banach   
\newcommand{\RN}{{{\mathbb R}^N}} 
\newcommand{\bR}{{{\mathbb R}}} 

\newcommand{\m}{\mathfrak{m}}
\newcommand{\toL}{\xrightarrow[]{{\mathcal L}}}
\newcommand{\toweak}{\xrightharpoonup[]{{\mathcal L}}}

\newcommand{\ps}[1]{\langle #1 \rangle}

% 
% Almost sure convergence
\newcommand{\toasshort}{\stackrel{\text{as}}{\to}}
\newcommand{\toaslong}{\xrightarrow[n\to\infty]{\text{a.s.}}}

% Convergence in probability 
\newcommand{\toprobashort}{\,\stackrel{\mathcal{P}}{\to}\,}
\newcommand{\toprobalong}{\xrightarrow[n\to\infty]{\mathcal P}}
%
% Convergence in law 
\newcommand{\todistshort}{{\stackrel{\mathcal{D}}{\to}}}
\newcommand{\todistlong}{\xrightarrow[n\to\infty]{\mathcal D}}

\usepackage[textwidth=2cm, textsize=footnotesize]{todonotes}  
\setlength{\marginparwidth}{1.5cm}               %  this goes with todonotes
\newcommand{\pbnote}[1]{\todo[color=cyan!20]{#1}}
\newcommand{\asnote}[1]{\todo[color=green!20]{#1}}
\newcommand{\whnote}[1]{\todo[color=magenta]{#1}}
\newcommand{\wh}[1]{{\color{red} #1}}

%Moreau
\newcommand{\my}{{{\nabla ^\gamma g}}}
\newcommand{\myn}{{{\nabla ^{\gamma_{n+1}} g}}}
%% ==============================================================

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{condition}{Condition}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}


% Title.
% ------

\title{Non asymptotic analysis of stochastic primal dual methods}

%
% Single address.
% ---------------

%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
%\ninept
%
\maketitle
%


%%Taux ps comme atcahdé
%%et en esperance (classic)
%%Qté sommable dans the theoreme de su candes
%parler approx sto et de nos articles

\begin{abstract} 
We prove linear convergence of the stochastic Forward Backward algorithm in Wasserstein distance toward its invariant measure under some strong monotonicity assumption. The rate can be arbitrarily close to zero. These results apply to various algorithms including Chambolle Pock.
\end{abstract}

%
%

\section{Introduction}
Finding a minimizer of a convex function or a saddle point of a convex concave function can be formulated as finding a zero of a maximal monotone operator. A standard method to find a zero of a monotone operator is the Proximal Point algorithm. The Proximal Point algorithm has many instances among minimization and saddle points methods including (in order of generality) proximal descent, Douglas Rachford, ADMM, Chambolle Pock.

The stochastic Proximal Point algorithm can be applied to find a zero of a monotone operator written as an expectation. It admits as instances stochastic versions of the algorithms aforementioned. The iterates of the stochastic Proximal Point algorithm ban be seen as a Markov Chain. Under a strong monotonicity assumption (that boils down to a strong convexity assumption in the particular cases), we prove that this Markov Chain admits an unique invariant measure, and prove linear convergence to this invariant measure in Wasserstein distance. We can identify this invariant measure in the case of the Basic method~\cite{richtarik2017stochastic}, it is the Dirac at at a solution. In the general case, we can bound the distance between the invariant measure and the Dirac at a minimizer. 
Stochastic version of Primal dual algorithm has been considered e.g. in~\cite{chambolle2018stochastic}. Linear convergence of stochastic optimization algorithm in Wasserstein distance has been considered in~\cite{dieuleveut2017bridging,can2019accelerated}. 


The use of the Wasserstein distance is not really a contribution, but just a way to express the result which is linear convergence up to the neighborhood. We found that this is cleaner. The Wasserstein distance allows to first express the linear rate to "something" and then express the convergence of the "something" to the solution as the step size goes to zero. 

\section{Basic Method}

The Basic Method is a random algorithm to solve the linear system $M x = b$. 

The algorithm can be written

It is known that the iterated of the algorithm satisfy

\begin{align*}
    \|x_{n+1} - x_\star\|^2 =& \|x_{n} - x_\star\|^2 \\
    &- 2\gamma\ps{g_{n+1},x_n - x_\star} + \gamma^2\|g_{n+1}\|^2\\
    =& \|x_{n} - x_\star\|^2 \\
    &- 2\gamma\ps{g_{n+1} - 0,x_n - x_\star} + \gamma^2\|g_{n+1} - 0\|^2\\
\end{align*}

Denoting $\mu_n$ the distribution of the random variable $x_n$, it can be seen that\asnote{to complete}

where $W$ is the Wasserstein distance of order 2 and $\delta_x$ is the Dirac measure at point $x$.

\section{Stochastic Forward Backward}

We now extend present the stochastic Forward Backward algorithm. 

\subsection{Maximal monotone operators}
An operator $\sA$ over $H$ is defined as a set valued mapping over $H$, \textit{i.e} a function from $H$ to the set of all subsets of $H$. An operator can be identified to its graph $G(\sA) = \{(x,y) \in H \times H, y \in \sA(x)\}$. The inverse operator $\sA^{-1}$ is defined by $G(\sA) = \{(x,y) \in H \times H, y \in \sA(x)\}$ and the identity map over $H$ is denoted $I$. Given $\lambda \geq 0$, the operator $\sA$ is called $\lambda$-monotone if the following condition holds: 
\begin{equation}
\forall (x,y),(x',y') \in G(\sA), \ps{x-x',y-y'} \geq \lambda \|x-x'\|^2,
\end{equation}
where $\ps{\cdot,\cdot}$ denotes the inner product of $H$. The operator $\sA$ is called monotone if there exists $\lambda \geq 0$ such that $\sA$ is $\lambda$-monotone, and strongly monotone if $\lambda > 0$.

Moreover, $\sA$ is a maximal monotone operator if $\sA$ is a monotone operator such that $G(\sA)$ is a maximal element (for the inclusion ordering) in the set of all graphs of monotone operators over $H$. It is known that maximal monotone operators are monotone operators for which, for every $\gamma > 0$, $J_{\gamma \sA} = (I + \gamma \sA)^{-1}$ is single valued, \textit{i.e} can be identified with a classical function from $H$ to $H$.

\subsection{Random monotone operators}
A random monotone operator $A$ is a maximal monotone operator depending on a random variable, \textit{i.e} $A(\cdot,\xi)$ where $\xi$ a random variable. For measurability issues, see~\cite{bia-hac-16}. A classical example is a random subdifferential, $\partial g(\cdot,\xi)$. 
\subsection{Stochastic Proximal Point algorithm}

The stochastic Proximal Point algorithm can be written
\begin{equation}
\label{eq:spp}
    x_{n+1} = J_{\gamma A(\cdot,\xi_{n+1})}(x_n)
\end{equation}
where $\gamma >0$ and $(\xi_n)$ is a sequcen of iid rv.
We shall rather use the following representation (that is a consequence of the definition of the resolvent): For every $n$, $x_{n+1}$ satisfies
\begin{equation}
    x_{n+1} \in x_n - \gamma A(x_{n+1},\xi_{n+1}).
\end{equation}

\section{Main result}
\begin{assumption}
$A(\cdot,\xi)$ is $\lambda(\xi)$-monotone
\end{assumption}

\begin{lemma}
$(x_n)$ is a Markov chain and admits an unique invariant measure $\pi^\gamma$.
\end{lemma}
\begin{proof}
Later. See~\cite{duf-livre97}.
\end{proof}

\begin{proposition}
Let be $\mu_n^\gamma$ the distribution of $x_n$ and assume that $A(\cdot,\xi)$ is $\lambda(\xi)$-monotone. Then,
\begin{equation}
    W^2(\mu_{n+1},\pi^\gamma)^2 \leq \bE_\xi\left(\frac{1}{1+2\gamma\lambda(\xi)}\right) W^2(\mu_{n},\pi^\gamma)^2
\end{equation}
Moreover, \begin{equation}
    \label{eq:cvd}
    \bE_\xi\left(\frac{1}{1+2\gamma\lambda(\xi)}\right) \longrightarrow_{\gamma \to \infty} \bP(\lambda(\xi) = 0)
\end{equation} 
\end{proposition}

\begin{proof}
Eq.~\eqref{eq:cvd} follows from dominated convergence theorem.
Now, consider another sequence $(y_n)$ produced by the stochastic proximal point algorithm, with the same noise $\xi_n$:
\begin{equation}
    y_{n+1} = J_{\gamma A(\cdot,\xi_{n+1})}(y_n)
\end{equation}

We have 
\begin{align}
    \frac{x_{n+1} - x_n}{\gamma} &\in - A(x_{n+1},\xi_{n+1}) \label{eq:x}\\
    \frac{y_{n+1} - y_n}{\gamma} &\in - A(y_{n+1},\xi_{n+1}) \label{eq:y}.
\end{align}
We first write
\begin{align}
    \|x_{n+1} - y_{n+1}\|^2 = &\|x_{n} - y_n\|^2 - \|(x_{n+1} - x_n) - (y_{n+1} - y_n)\|^2 \label{eq:square}\\
    &-2\gamma\ps{\frac{x_n - x_{n+1}}{\gamma} - \frac{y_n - y_{n+1}}{\gamma}, x_{n+1} - y_{n+1}}
\label{eq:inner}
\end{align}
Noting that $\frac{x_n - x_{n+1}}{\gamma} \in A(x_{n+1},\xi_{n+1})$ and $\frac{y_n - y_{n+1}}{\gamma} \in A(y_{n+1},\xi_{n+1})$, we have
\begin{equation}
    \lambda(\xi_{n+1})\|x_{n+1} - y_{n+1}\|^2 \leq \ps{\frac{x_n - x_{n+1}}{\gamma} - \frac{y_n - y_{n+1}}{\gamma}, x_{n+1} - y_{n+1}}
\end{equation}
and
\begin{equation}
    \|(x_{n+1} - x_n) - (y_{n+1} - y_n)\|^2 = \gamma^2\|\frac{x_{n+1} - x_n}{\gamma} - \frac{y_{n+1} - y_n}{\gamma}\|^2 \geq \gamma^2\lambda(\xi_{n+1})\|x_{n+1} - y_{n+1}\|^2.
\end{equation}
Therefore, 
\begin{equation}
    \|x_{n+1} - y_{n+1}\|^2 \leq \frac{1}{1+2\lambda(\xi_{n+1})\gamma + \lambda(\xi_{n+1})\gamma^2}\|x_{n} - y_{n}\|^2.
\end{equation}
Assuming that $y_0 \sim \pi^\gamma$ and taking expectation,

\begin{equation}
    \bE(\|x_{n+1} - y\|^2) \leq \bE\left(\frac{1}{1+2\gamma\lambda(\xi) + \lambda(\xi)\gamma^2}\right) \bE(\|x_{n} - y\|^2),
\end{equation}
where $y$ is any rv with distribution $\pi^\gamma$.
Taking optimal couplings we get the result.
\end{proof}
\section{Application}

\subsection{Stochastic proximal descent}

Problem:
\begin{equation}
    \min_x \bE(g(x,\xi))
\end{equation}
where $g(\cdot,s) \in \Gamma_0(\bR^d)$.

The algorithm:
\begin{equation}
\label{eq:spg}
    x_{n+1} = \prox_{\gamma g(\cdot,\xi_{n+1})}(x_n)
\end{equation}
is in instance of~\eqref{eq:spp} with $A(x,\xi) = \partial g(x,\xi)$

\begin{corollary}
Assume that $g(\cdot,\xi)$ is $\lambda(\xi)$-(strongly) convex. Then,
\begin{equation}
    W^2(\mu_{n+1},\pi^\gamma)^2 \leq \bE_\xi\left(\frac{1}{1+2\gamma\lambda(\xi)+\lambda(\xi)\gamma^2}\right) W^2(\mu_{n},\pi^\gamma)^2
\end{equation}
\end{corollary}


\subsection{Chambolle Pock}

Problem:
\begin{equation}
    \min_x \bE(g(x,\xi)) + H(Lx)
\end{equation}
where $g(\cdot,s) \in \Gamma_0(\bR^d)$ and $L : \bR^d \to \bR^p$ matrix. Moreover, $H \in \Gamma_0(\bR^p)$ and $H^\star(y) = \bE(p(y,\xi))$ where $p(\cdot,\xi) \in \Gamma_0(\bR^p)$.

Algorithm:
\begin{align}
    x_{n+1} &= \prox_{\gamma g(\cdot,\xi)}(x_n - \gamma L^{T} y_n)\\
    y_{n+1} &= \prox_{\gamma p(\cdot,\xi)}(y_n + \gamma L (2 x_{n+1} - x_n)).
\end{align}

If $L = I$, then it is stochastic Douglas Rachford/ADMM.



\begin{corollary}
Denote $\mu_n$ the distribution of $(x_n,y_n)$ and $\pi^\gamma$ its invariant measure. Assume that $g(\cdot,\xi)$ is $\lambda_1(\xi)$-(strongly) convex and $p(\cdot,\xi)$ is $\lambda_2(\xi)$-smooth. Then,
\begin{equation}
    W^2(\mu_{n+1},\pi^\gamma)^2 \leq \bE_\xi\left(\frac{1}{1+2\gamma\lambda(\xi)+\lambda(\xi)\gamma^2}\right) W^2(\mu_{n},\pi^\gamma)^2,
\end{equation}
where $\lambda(\xi) = \min(\lambda_1(\xi),\lambda_2(\xi)).$
\end{corollary}



\section{Invariant measures}

The neighborhood information is contained in the value of the\footnote{our analysis will show that there is only one invariant measure} invariant measure $\pi_\gamma$.
For example, for basic method the neighborhood is reduced to the solution and so $\pi_\gamma = \delta_{x_\star}$.

\begin{theorem}
    If $A(\xi)$ is $\lambda$-strongly monotone ($\lambda$ deterministic?) then
    \begin{equation}
        W^2(\pi_\gamma,\delta_{x_\star}) \leq \frac{\gamma \bE(\|\phi(\xi)\|^2)}{2\lambda + \gamma\lambda}.
    \end{equation}
    where $\phi(\xi) \in A(x_\star,\xi)$ a.s.
\end{theorem}
\begin{proof}
    Let $x \in H$ and $x^+ = J_{\gamma A(\xi)}(x)$. First,
    \begin{align*}
        \|x^+ - x_\star\|^2 &= \|x-x_\star\|^2 - 2\gamma\ps{A_\gamma(x,\xi),x-x_\star} + \gamma^2\|A_\gamma(x,\xi)\|^2,
    \end{align*}
    where
    \begin{align*}
        \ps{A_\gamma(x,\xi),x-x_\star} &= \ps{A_\gamma(x,\xi)-\phi(\xi),x-x_\star} + \ps{\phi(\xi),x-x_\star}\\
        &\ps{A_\gamma(x,\xi)-\phi(\xi),x^+ - x_\star} + \ps{A_\gamma(x,\xi)-\phi(\xi),x - x^+} + \ps{\phi(\xi),x-x_\star}\\
        &\ps{A_\gamma(x,\xi)-\phi(\xi),x^+ - x_\star} + \ps{A_\gamma(x,\xi)-\phi(\xi),\gamma A_\gamma(x,\xi)} + \ps{\phi(\xi),x-x_\star}.
    \end{align*}
    Therefore,
    \begin{align*}
        \|x^+ - x_\star\|^2 &= \|x-x_\star\|^2 - 2\gamma\ps{A_\gamma(x,\xi) - \phi(\xi),x^+-x_\star} + \gamma^2\|A_\gamma(x,\xi)\|^2\\
        &-2\gamma\ps{A_\gamma(x,\xi)-\phi(\xi),\gamma A_\gamma(x,\xi)} - 2\gamma \ps{\phi(\xi),x-x_\star}\\
        &= \|x-x_\star\|^2 - 2\gamma\ps{A_\gamma(x,\xi) - \phi(\xi),x^+-x_\star} - \gamma^2\|A_\gamma(x,\xi) - \phi(\xi)\|^2\\
        &- 2\gamma \ps{\phi(\xi),x-x_\star} + \gamma^2\|\phi(\xi)\|^2
    \end{align*}
Since $\phi(\xi) \in A(x_\star,\xi)$ we can now use our only allowed inequality (strong monotonicity) 

\begin{align*}
    \|x^+ - x_\star\|^2 &\leq \frac{1}{1 + 2\gamma \lambda + \gamma^2\lambda}\|x-x_\star\|^2 - \frac{2\gamma}{1 + 2\gamma \lambda + \gamma^2\lambda} \ps{\phi(\xi),x-x_\star} + \frac{\gamma^2}{1 + 2\gamma \lambda + \gamma^2\lambda}\|\phi(\xi)\|^2.
\end{align*}
Assume that $x \sim \pi_\gamma$, then, $x^+ \sim \pi_\gamma$ and by taking optimal couplings
\begin{align*}
    W^2(\pi_\gamma,\delta_{x_\star}) &\leq \frac{1}{1 + 2\gamma \lambda + \gamma^2\lambda}W^2(\pi_\gamma,\delta_{x_\star}) + \frac{\gamma^2}{1 + 2\gamma \lambda + \gamma^2\lambda}\bE_\xi(\|\phi(\xi)\|^2).
\end{align*}
Finally,
\begin{equation}
    W^2(\pi_\gamma,\delta_{x_\star}) \leq \frac{\gamma \bE(\|\phi(\xi)\|^2)}{2\lambda + \gamma\lambda}.
\end{equation}

\subsection{Basic method}

Does the rate match the one of Basic method?
Basic method with step size $\omega$ has linear rate $(1-\omega(2-\omega)\lambda_m)$ (cf. Nicolas' thesis).
Basic method can be seen as a stochastic proximal point algorithm with step size $\gamma = \frac{\omega}{1-\omega}$.
What is the rate obtained here? It is 
$$
\frac{1}{1+2\gamma\lambda+\gamma^2\lambda} = \frac{1}{1+2(\frac{\omega}{1-\omega})\lambda+(\frac{\omega}{1-\omega})^2\lambda}
$$
\asnote{To be compared...}

\subsection{More generally}



\section{Extension}

We can extend the result on~\ref{eq:spp} to the Forward Backward algorithm. It allows to cover Vu-Condat, the stochastic proximal gradient descent and the algorithm presented in~\cite{salim2018splitting}.




Rewriting the inner product~\eqref{eq:inner}
\begin{align}
&\ps{\frac{x_n - x_{n+1}}{\gamma} - \frac{y_n - y_{n+1}}{\gamma}, x_{n+1} - y_{n+1}} \nonumber\\
= & \ps{B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1}), x_{n+1} - y_{n+1}} 
\label{eq:inner2}\\
&+  \ps{A_\gamma(x_n - \gamma B(x_n,\xi_{n+1}),\xi_{n+1}) - A_\gamma(y_n - \gamma B(y_n,\xi_{n+1}),\xi_{n+1}), x_{n+1} - y_{n+1}}\nonumber.
\end{align}
Rewriting the inner product~\eqref{eq:inner2}
\begin{align*}
    &\ps{B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1}), x_{n+1} - y_{n+1}} \\
    =& \ps{B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1}), x_{n} - y_{n}} \\
    &+ \ps{B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1}), (x_{n+1}-x_n) - (y_{n+1}-y_n)}\\
    =& \ps{B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1}), x_{n} - y_{n}} \\
    &-\gamma \ps{B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1}), B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1})}\\
    &-\gamma \ps{B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1}), A_\gamma(x_n - \gamma B(x_n,\xi_{n+1}),\xi_{n+1}) - A_\gamma(y_n - \gamma B(y_n,\xi_{n+1}),\xi_{n+1})}.
\end{align*}
Plugging everything into~\eqref{eq:square}
\begin{align*}
    &\|x_{n+1} - y_{n+1}\|^2 \\
    = &\|x_{n} - y_n\|^2 - \|(x_{n+1} - x_n) - (y_{n+1} - y_n)\|^2 \\
    &-2\gamma \ps{A_\gamma(x_n - \gamma B(x_n,\xi_{n+1}),\xi_{n+1}) - A_\gamma(y_n - \gamma B(y_n,\xi_{n+1}),\xi_{n+1}), x_{n+1} - y_{n+1}}\\
    &-2\gamma\ps{B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1}), x_{n} - y_{n}}\\
    &+2\gamma^2 \|B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1})\|^2\\
    &+2\gamma^2 \ps{B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1}), A_\gamma(x_n - \gamma B(x_n,\xi_{n+1}),\xi_{n+1}) - A_\gamma(y_n - \gamma B(y_n,\xi_{n+1}),\xi_{n+1})}.
\end{align*}
Note the relationship between the second term at the right hand side, and the last two terms:
\begin{align}
    &\|(x_{n+1} - x_n) - (y_{n+1} - y_n)\|^2 \\
    =&\gamma^2 \|B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1})\|^2\\
    &+\gamma^2 \|A_\gamma(x_n - \gamma B(x_n,\xi_{n+1}),\xi_{n+1}) - A_\gamma(y_n - \gamma B(y_n,\xi_{n+1}),\xi_{n+1})\|^2\\
    &+2\gamma^2 \ps{B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1}), A_\gamma(x_n - \gamma B(x_n,\xi_{n+1}),\xi_{n+1}) - A_\gamma(y_n - \gamma B(y_n,\xi_{n+1}),\xi_{n+1})}.
\end{align}
Therefore,
\begin{align*}
    &\|x_{n+1} - y_{n+1}\|^2 \\
    = &\|x_{n} - y_n\|^2\\
    &-2\gamma \ps{A_\gamma(x_n - \gamma B(x_n,\xi_{n+1}),\xi_{n+1}) - A_\gamma(y_n - \gamma B(y_n,\xi_{n+1}),\xi_{n+1}), x_{n+1} - y_{n+1}}\\
    &-2\gamma\ps{B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1}), x_{n} - y_{n}}\\
    &+\gamma^2 \|B(x_n,\xi_{n+1}) - B(y_n,\xi_{n+1})\|^2\\
    &-\gamma^2 \|A_\gamma(x_n - \gamma B(x_n,\xi_{n+1}),\xi_{n+1}) - A_\gamma(y_n - \gamma B(y_n,\xi_{n+1}),\xi_{n+1})\|^2.
\end{align*}
We now start to use our assumptions. Note that we didn't use any inequality so far. 
\textbf{Case 1: $B$ cocoercive.}


\textbf{Case 1: $B$ Lipschitz.}
\asnote{$A$ must be strongly monotone with a deterministic constant but for be we only assume $\bE(B)$ strongly monotone}


\section{improvement grad prox}

\begin{proposition}
Assume that $F$ is $\mu$-strongly convex and stochastically cocoercive in the sense that \asnote{actually we only need this assumption with $x' = x_\star$}
\begin{equation}
\label{eq:cocoercivity}
    \exists L >0, \forall x,x', \in \bR^d \quad \bE_\zeta \|g(x,\zeta) - g(x',\zeta)\|^2 \leq L  \bE_\zeta \ps{g(x,\zeta) - g(x',\zeta),x-x'}.
\end{equation}
Assume moreover that $\bE_\zeta(\|\nabla f(x_\star,\zeta)\|^2) < \infty$ where $x_\star$ is the solution to problem~\eqref{eq:constrained} and that 
\begin{equation}
\label{eq:qualif}
\bigcap_{i = 1}^p ri(C_i) \neq \emptyset,
\end{equation}
(where $ri$ is the relative interior). Then,
the map $\cT_{RPSG}$ satisfies Assumption~\ref{ass:iterative_shrinkage} with $\rho = (1-2\gamma(1-\gamma L)\mu)$ and $\eta = 2\gamma^2\sigma^2$.
\end{proposition}
\begin{proof}
\asnote{We should be able to obtain this result as a consequence of Wang and Bertsekas but I have to double check.}
Before entering the proof, we need to provide some material from nonsmooth convex analysis. Given a convex lower semicontinuous and proper function $h$ over $\bR^d$ (we shall write $h \in \Gamma_0(\bR^d)$), its subdifferential is denoted $\partial h$. It is known that $\partial h$ is a monotone operator in the sense that $\ps{x-x',y-y'}$ for every $y \in \partial h(x)$ and $y' \in partial h(x')$. It is also known that if $h$ is differentiable, then $\partial h = \{\nabla h\}$. Given $\gamma >0$, the proximity operator of $\gamma h$ is defined as $\prox_{\gamma h}(x) = \argmin_{y} \frac{1}{2\gamma}\|y - x\|^2 + h(y)$ and its Moreau envelope $^\gamma h(x) = \min_{y} \frac{1}{2\gamma}\|y - x\|^2 + h(y)$. The Moreau envelope is known to be a $1/\gamma$-smooth function. Two important relationship links the gradient of the Moreau envelope and the proximity operator. First, $\nabla ^\gamma h(x) = \frac{1}{\gamma}(x - \prox_{\gamma h}(x))$. Second, $\nabla ^\gamma h(x) \in \partial h(\prox_{\gamma h}(x))$. Given $h_1, h_2 \in \Gamma_0(\bR^d)$, it is not true in general that $\partial (h_1+h_2) = \partial h_1 + \partial h_2$ where the symbol $+$ is the classical sum between sets. However, this equality is true under some qualification conditions that we shall use in the sequel. It is easy to check that if $h = \iota_\mC$ the indicator function of $\mC$, then $\partial \iota_\mC = N_\mC$ (the normal cone to $\mC$), $\prox_{\iota_\mC} = \proj_{\mC}$ and $\iota_\mC = \sum_{i=1}^p \iota_{C_i}$. Moreover, under our qualification condition~\eqref{eq:qualif}, it is known that $\partial \iota_\mC = \sum_{i=1}^p \partial \iota_{C_i}$, \textit{i.e} $N_\mC = \sum_{i=1}^p N_{C_i}$. Finally, the problem~\eqref{eq:constrained} being equivalent to $\min F + \iota_\mC$, its solution $x_\star$ is equivalently a solution to the inclusion $0 \in \partial (F + \partial \iota_\mC)(x_\star)$. Since $F$ is smooth, $\partial (F + \iota_\mC) = \partial F + \partial \iota_\mC$ \textit{i.e} $\partial (F + \iota_\mC) = \nabla F + \sum_{i=1}^p N_{C_i}.$ By definition of the normal cone, we also have the equality $N_\mC = \sum_{i=1}^p p_i N_{C_i}$. Therefore, $x_\star$ is a solution to problem~\eqref{eq:constrained} if and only if $0 \in \nabla F(x_\star) + \sum_{i=1}^p p_i N_{C_i}(x_\star)$ \textit{i.e} if and only if for every $i$, there exists $\phi_i \in N_{C_i}(x_\star)$ such that $\phi_i \in N_{C_i}(x_\star)$ and $0 = \nabla F(x_\star) + \sum_{i=1}^p p_i \phi_i$. This can be finally rewritten 
\begin{equation}
\label{eq:rpz-zero}
0 = \bE_{(\zeta,I)}(g(x_\star,\zeta) + \phi_I).
\end{equation}

Let us denote $x^+ = \cT_{RPSG}(y)$ and $h(y,i) = \iota_{C_i}(y)$. Note that $\cT_{RPSG}(y) = y - \gamma g(y,\zeta) - \gamma \nabla ^\gamma h(y - \gamma g(y,\zeta),I)$. In the sequel we use the notation $\nabla ^\gamma h \equiv \nabla ^\gamma h(y - \gamma g(y,\zeta),I)$. First,
\begin{equation}
\label{eq:expand-square}
    \|x^+ - x_\star\|^2 \leq \|y - x_\star\|^2 -2\gamma\ps{g(y,\zeta) + \nabla ^\gamma h, y - x_\star} + \gamma^2\|g(y,\zeta) + \nabla ^\gamma h\|^2.
\end{equation}
Then we write
\begin{equation*}
    \ps{g(y,\zeta),y-x_\star} = \ps{g(y,\zeta) - g(x_\star,\zeta),y-x_\star} + \ps{g(x_\star,\zeta),y-x_\star},
\end{equation*}
and
\begin{align*}
    &\ps{\nabla ^\gamma h,y-x_\star} \\=& \ps{\nabla ^\gamma h - \phi_I,y-x_\star} + \ps{\phi_I,y-x_\star}\\
    =&\ps{\nabla ^\gamma h - \phi_I,x^+-x_\star} + \ps{\phi_I,y-x_\star} + \ps{\nabla ^\gamma h - \phi_I,y - x^+- \gamma g(y,\zeta)} + \ps{\nabla ^\gamma h - \phi_I, \gamma g(y,\zeta)}\\
    =&\ps{\nabla ^\gamma h - \phi_I,x^+-x_\star} + \ps{\phi_I,y-x_\star} + \ps{\nabla ^\gamma h - \phi_I,\gamma \nabla ^\gamma h} + \ps{\nabla ^\gamma h - \phi_I, \gamma g(y,\zeta)}.
\end{align*}
Plugging into~\eqref{eq:expand-square},
\begin{align*}
\label{eq:expand-square}
    &\|x^+ - x_\star\|^2 \\ 
    =& \|y - x_\star\|^2 -2\gamma \left(\ps{g(y,\zeta) - g(x_\star,\zeta),y-x_\star} + \ps{\nabla ^\gamma h - \phi_I,x^+-x_\star} - \ps{g(x_\star,\zeta)+ \phi_I,y-x_\star}\right)\\
    &+ \gamma^2\|g(y,\zeta)\|^2 + 2\gamma^2\ps{g(y,\zeta),\nabla ^\gamma h} + \gamma^2\|\nabla ^\gamma h\|^2 - 2\gamma^2\|\nabla ^\gamma h\|^2 - 2\gamma^2\ps{g(y,\zeta),\nabla ^\gamma h}\\
    &+ 2\gamma^2\ps{\phi_I,g(y,\zeta)+\nabla ^\gamma h}\\
    =& \|y - x_\star\|^2 -2\gamma \left(\ps{g(y,\zeta) - g(x_\star,\zeta),y-x_\star} + \ps{\nabla ^\gamma h - \phi_I,x^+-x_\star} - \ps{g(x_\star,\zeta)+ \phi_I,y-x_\star}\right)\\
    &+ \gamma^2\|g(y,\zeta) + \phi_I\|^2 - \gamma^2\|\nabla ^\gamma h - \phi_I\|^2.
\end{align*}
Since $\|\nabla ^\gamma h - \phi_I\|^2$ and $\ps{\nabla ^\gamma h - \phi_I,x^+-x_\star}$ are nonnegative (monotonicity of $\partial \iota_{C_I}$), and using Equation~\eqref{eq:rpz-zero} we have
\begin{equation*}
    \bE \|x^+ - x_\star\|^2 \leq \|y - x_\star\|^2 -2\gamma \ps{\nabla F(y) - \nabla F(x_\star),y-x_\star} + \gamma^2 \bE \|g(y,\zeta) + \phi_I\|^2.
\end{equation*}
Now we write $g(y,\zeta) + \phi_I = (g(y,\zeta) - g(x_\star,\zeta)) + (g(x_\star,\zeta) + \phi_I)$ and successively use Young's inequality and~\eqref{eq:cocoercivity}.
\begin{align*}
    \bE \|x^+ - x_\star\|^2 \leq& \|y - x_\star\|^2 -2\gamma \ps{\nabla F(y) - \nabla F(x_\star),y-x_\star}\\
    &+ 2\gamma^2 \bE \|g(x_\star,\zeta) + \phi_I\|^2 + 2\gamma^2 \bE\|g(y,\zeta) - g(x_\star,\zeta)\|^2\\
    \leq& \|y - x_\star\|^2 -2\gamma(1-\gamma L) \ps{\nabla F(y) - \nabla F(x_\star),y-x_\star}\\
    &+ 2\gamma^2 \bE \|g(x_\star,\zeta) + \phi_I\|^2\\
    \leq& \|y - x_\star\|^2 -2\gamma(1-\gamma L)\mu \|y-x_\star\|^2\\
    &+ 2\gamma^2 \bE \|g(x_\star,\zeta) + \phi_I\|^2.
\end{align*}
Denoting $\sigma^2 = \bE \|g(x_\star,\zeta) + \phi_I\|^2$ the variance of the (zero mean) random variable $g(x_\star,\zeta) + \phi_I$, we finally have
\begin{equation}
\label{eq:linear-neighborhood-rpsg}
    \bE \|x^+ - x_\star\|^2 \leq (1-2\gamma(1-\gamma L)\mu)\|y - x_\star\|^2 + 2\gamma^2 \sigma^2. 
\end{equation}
\end{proof}

\section{Weakly convex case}

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------

We want to solve 
\begin{equation}
    \min_x \bE_\xi(g(x,\xi)) \quad\text{s.t.}\quad \bE(L)x = \bE(b),
\end{equation}
where $L$ is a random matrix and $b$ a random vector. We start with $b = 0$ and $L$ deterministic. 

Chambolle Pock for this problem is written
\begin{align}
    x_{n+1} &= \prox_{\gamma g(\cdot,\xi_{n+1})}(x_n - \gamma L_{n+1}^T y_n)\\
    y_{n+1} &= y_n + \gamma L_{n+1}(2 x_{n+1} - x_n).
\end{align}
Let $z_n = (x_n, y_n)$, 
\[
P_\gamma = \begin{bmatrix} I &  -\gamma L^T \\ -\gamma L & I \end{bmatrix},
\]
\[
A(z,\xi) = \begin{bmatrix} \partial f(x,\xi)&  + L^T y \\ -L x& \end{bmatrix} .
\]

Algorithm:

$$
P_\gamma(z_{n+1} - z_n) \in -\gamma A(z_{n+1}) 
$$


\begin{align*}
&\|z_{n+1} - z_\star\|_\gamma^2 \\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 + \ps{z_n - z_\star,(P_{n+1} - P_n)z_n - z_\star}\\
& -2\gamma \ps{A(z_{n+1}) - \varphi,z_{n+1} - z_\star} -2\gamma \ps{\varphi,z_{n} - z_\star} -2\gamma \ps{\varphi,z_{n+1} - z_n}\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 +  \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C\\
& -2\gamma \alpha \ps{A(z_{n+1}) - \varphi,z_{n+1} - z_\star} \\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 +  \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C\\
& -2\gamma\alpha \left(( g(x_{n+1},\xi_{n+1}) + \ps{L x_{n+1} - b,y_\star} ) - ( g(x_\star,\xi_{n+1}) + \ps{L x_\star - b,y_{n+1}} )\right)\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 +  \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C\\
& -2\gamma\alpha \left(( g_\gamma(x_{n} - \gamma L^T y_{n},\xi_{n+1}) + \ps{L x_{n}-b,y_\star} ) - ( g(x_\star,\xi_{n+1}) + \ps{L x_\star-b,y_{n}} )\right)\\
& -2\gamma\alpha \left(-\frac{1}{2\gamma}\|x_{n+1} - x_n - \gamma L^T y_n\|^2 + \ps{L (x_{n+1} - x_n),y_\star} - \ps{L x_\star - b,y_{n+1} - y_n}\right)\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 + \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C \\
& -2\gamma\alpha \left(( g_\gamma(x_{n} - \gamma L^T y_{n},\xi_{n+1}) + \ps{L x_{n}-b,y_\star} ) - ( g(x_\star,\xi_{n+1}) + \ps{L x_\star-b,y_{n}} )\right)\\
& + \alpha\|x_{n+1} - x_n - \gamma L^T y_n\|^2.\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 + \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C \\
& -2\gamma\alpha \left( g_\gamma(x_{n},\xi_{n+1}) + \ps{\nabla^\gamma g(x_n-\gamma L^T y_n,\xi_{n+1}), -\gamma L^T y_n} - \frac{\gamma}{2}\|L^T y_n\|^2 + \ps{L x_{n}-b,y_\star} \right) \\
& +2\gamma \alpha \left( g(x_\star,\xi_{n+1}) + \ps{L x_\star -b,y_{n}} \right)\\
& + \alpha\|x_{n+1} - x_n - \gamma L^T y_n\|^2\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 + \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C \\
& -2\gamma\alpha \left( g(x_{n},\xi_{n+1}) - \frac{\gamma}{2}\left(\|\nabla^0 g(x_{n},\xi_{n+1})\|^2 - \|\nabla^\gamma g(x_n-\gamma L^T y_n,\xi_{n+1})\|^2\right) + \ps{L x_{n}-b,y_\star} \right)\\
&-2\gamma\alpha \left(-\frac{\gamma}{2}\|\nabla^\gamma g(x_n-\gamma L^T y_n,\xi_{n+1})\|^2 - \gamma\ps{\nabla^\gamma g(x_n-\gamma L^T y_n,\xi_{n+1}), L^T y_n} - \frac{\gamma}{2}\|L^T y_n\|^2 \right) \\
& +2\gamma \alpha\left( g(x_\star,\xi_{n+1}) + \ps{L x_\star-b,y_{n}} \right)\\
& +\alpha \|x_{n+1} - x_n - \gamma L^T y_n\|^2\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 + \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C \\
& -2\gamma\alpha \left( g(x_{n},\xi_{n+1}) - \frac{\gamma}{2}\|\nabla^0 g(x_{n},\xi_{n+1})\|^2 + \ps{L x_{n}-b,y_\star} \right)\\
& + \alpha\|x_{n+1} - x_n\|^2 \\
& +2\gamma \alpha\left( g(x_\star,\xi_{n+1}) + \ps{L x_\star-b,y_{n}} \right)\\
& + \alpha\|x_{n+1} - x_n - \gamma L^T y_n\|^2 - \alpha\gamma^2 \|\nabla^\gamma g(x_n-\gamma L^T y_n,\xi_{n+1})\|^2\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 + \varepsilon \|z_{n+1} - z_{n}\|^2 + \gamma^2 C \\
& -2\gamma\alpha \left( g(x_{n},\xi_{n+1}) - \frac{\gamma}{2}\|\nabla^0 g(x_{n},\xi_{n+1})\|^2 + \ps{L x_{n}-b,y_\star} \right)\\
& + \alpha\|x_{n+1} - x_n\|^2 \\
& +2\gamma\alpha \left( g(x_\star,\xi_{n+1}) + \ps{L x_\star-b,y_{n}} \right)\\
\leq &\|z_{n} - z_\star\|_\gamma^2 - \|z_{n+1} - z_{n}\|_\gamma^2 + (\varepsilon + \alpha) \|z_{n+1} - z_{n}\|^2 + \gamma^2 C \\
& -2\gamma\alpha \left( g(x_{n},\xi_{n+1}) + \ps{L x_{n}-b,y_\star} \right)\\
& +2\gamma\alpha \left( g(x_\star,\xi_{n+1}) + \ps{L x_\star-b,y_{n}} \right).
\end{align*}
Taking $\varepsilon, \alpha$ enough small such that $(\varepsilon + \alpha) \|\cdot\|^2 \leq \|\cdot\|_\gamma^2$, and taking expectation we have

\begin{equation}
    \label{eq:final}
    \bE_n(\|z_{n+1} - z_\star\|_\gamma^2) \leq \|z_n - z_\star\|_\gamma^2 -2\gamma\alpha\left( \cL(x_n,y_\star) - \cL(x_\star,y_n)\right)+ \gamma^2 C
\end{equation}
\asnote{Add : domain, forward step}

\begin{equation}
z_{n+1} - z_n = \begin{bmatrix} x_{n+1} - x_{n} \\ y_{n+1} - y_{n} \end{bmatrix}
 = \gamma \begin{bmatrix} - \nabla^\gamma g(x_n - \gamma L^T y_n,\xi_{n+1}) & -L^T y_n \\ L (2 x_{n+1} - x_n)& \end{bmatrix}.
\end{equation}

\begin{align}
P_{\gamma} (z_{n+1} - z_n) &= \gamma \begin{bmatrix} - \nabla^\gamma g(x_n - \gamma L^T y_n,\xi_{n+1}) & -L^T \left(y_n +\gamma L (2 x_{n+1} - x_n)\right) \\ L \left(x_{n} -\gamma \nabla^\gamma g(x_n - \gamma L^T y_n,\xi_{n+1}) - \gamma L^T y_n\right) \end{bmatrix}\\
&= \gamma \begin{bmatrix} - \nabla^\gamma g(x_n - \gamma L^T y_n,\xi_{n+1}) & -L^T y_{n+1} \\ L x_{n+1} \end{bmatrix}\\
&\in \gamma \begin{bmatrix} - \partial g(x_{n+1},\xi_{n+1}) & -L^T y_{n+1} \\ L x_{n+1} \end{bmatrix}\\
\end{align}

\begin{align}
    \ps{(z_{n+1} - z_n), P_{\gamma} (z_{n+1} - z_n)} = 
\end{align}






\asnote{Ecrire la fonction duale comme inf convolution comme Combettes, pour linearizer dans le dual et avoir un operateur B sans zero entry (mettre un terme de l'inf convo dans B et un autre dans A)}





\section{Invariant measure}

In the weakly convex case we know that cluster points of invariant measures are invariant for the flow and hence supported by the solutions. Here, in the strongly convex case, we prove $W^2(\pi_\gamma,\pi_\star) = O(\gamma)$. 
\bibliography{math}

\end{document}
